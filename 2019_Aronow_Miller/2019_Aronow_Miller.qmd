---
title: "Foundations of Agnostic Statistics"
comments:
  giscus:
    repo: ctesta01/CausalFall2023
---

[‚Üê Home](https://ctesta01.github.io/CausalFall2023/)

::: {.content-hidden}
$$
\newcommand{\E}[0]{\mathbb E}

% 1 create conditionally independent symbol:
\newcommand\independent{\perp\!\!\!\perp}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
$$
:::

Notes on <i>Foundations of Agnostic Statistics</i> by Peter M. Aronow and Benjamin T. Miller (2019), Cambridge
University Press.

<https://doi.org/10.1017/9781316831762>

## Introduction 

For this week's reading, I jumped straight into the deep-end and read chapters 6 and 7 of 
Aronow and Miller's (relatively) new book.  I found their text extremely enjoyable and 
enlightening, and was especially excited to see their easy presentation of 
doubly-robust estimators, something I've been looking around for for a while. 

The other aspect that I particularly enjoyed was that they make the similarities 
between handling missing data and questions of causal inference as explicit as 
possible, going so far as to structure chapters 6 and 7 in parallel, 6 being 
about missing data and 7 being about causal inference. 

**Definition.** The stable outcomes model is for missing data 
is given 
$$Y_i^* = \left\{ \begin{array}{ll} -99 & R_i = 0 \\ Y_i & R_i = 1 \end{array} \right.$$
where $R_i$ is the missingness indicator variable. This can also be written as $Y_i^* = Y_i R_i + (-99)(1-R_i)$. 

This is called the *stable outcomes model* because 
it presumes that $Y_i$ is itself a stable quantity.

As far as I can tell, they're using $-99$ in the 
equations not because it makes the math go through, but
because $-99$ is number commonly used to indicate
missingness.

## Sharp Bounds for Estimates

**Theorem 6.1.3.** Sharp bounds for the expected value. 
Let $Y_i$ and $R_i$ be random variables 
with support for $Y_i$ be restricted to $[a,b]$,
$\text{support}(R_i) \in \{0,1\}$, and 
$Y_i*$ is as in the stable outcomes model. Then 

$$\begin{align} 
\E[Y_i] \in \big[
\E[Y_i^* | & R_i = 1]Pr(R_i=1) + aPr(R_i = 0), \\ 
& \E[Y_i^* | R_i = 1]Pr(R_i=1) + bPr(R_i = 0)\big]. \\
\end{align}$$

I found this books' proofs quite clear and easy 
to follow, so I am not going to re-write them here and
instead refer readers to the book.

Instead, I think some simulation exercise to build
our intuition on this would be nice. 

```{r}
N <- 1000
A <- rnorm(n = N)

# introduce the logistic function
logistic <- function(x) { exp(x) / (1+exp(x)) }
curve(logistic(x), from = -10, to = 10)

# simulate our true outcome
Y <- rnorm(n = N, mean = 2*A + A^2)

# simulate our missingness mechanism
R <- rbinom(n = N, size = 1, prob = logistic(A+Y))

# subset observed data
A_observed <- A[as.logical(1-R)]
Y_observed <- Y[as.logical(1-R)]

plot(A, Y)
plot(A_observed, Y_observed)

# estimate true regression equation
jtools::summ(lm(Y ~ A))

# missingness biased regression equation
jtools::summ(lm(Y_observed ~ A_observed))

# create stable_outcomes_model
Y_stable <- Y*R + (-99)*(1-R)

# lower bound on the A-Y effect
mean(Y_stable*R + min(Y_observed)*(1-R))

# upper bound on the A-Y effect
mean(Y_stable*R + max(Y_observed)*(1-R))
```

In chapter 7, they provide a similar theorem that provides the means to bound a causal effect. 
There is one crucial difference, though, which is that because the average treatment effect
is the difference between two potential outcomes (under treatment and under non-treatment), 
whereas the effect estimate in the missing data scenario is purely associational, the 
causal theorem has an additional subtracted term. 

**Theorem 7.1.17.** Sharp bounds for the average treatment effect. 

Let $Y_i(0)$, $Y_i(1)$, and $D_i$ be random variables such that 
$\forall d \in \{0, 1\},$ $\text{support}(Y_i(d)) \subset [a,b],$ and 
$\text{support}(D_i) = \{0,1\}$. Let $Y_i = Y_i(1) \cdot D_i + Y_i(0) \cdot (1-D_i)$ 
and $\tau_i = Y_i(1) - Y_i(0)$. Then 

$$\begin{aligned}\E[\tau_i] \in \big[ \E( & Y_i | D_i = 1)Pr(D_i=1) + aPr(D_i = 0) \\ 
&  -(\E(Y_i|D_i=0)Pr(D_i=0) + bPr(D_i=1)), \\
& \E( Y_i | D_i = 1)Pr(D_i=1) + bPr(D_i = 0) \\ 
& -(\E(Y_i|D_i=0)Pr(D_i=0) + aPr(D_i=1)) \big]. \end{aligned} $$ 


Let's simulate some data and run an analysis to build some intuition 
around how this sharp bounding works. For example, we might 
imagine a scenario where the treatment assigment and the outcome
are confounded, and hence the naive (unadjusted) estimate of the 
average treatment effect would be biased.

```{r, out.width='5in', fig.align = 'center'}
library(tidyverse)
N <- 1000

set.seed(1234)
L <- rnorm(n = N)
D <- rbinom(n = N, size = 1, prob = logistic(L))
Y1 <- L + rnorm(n = N, mean = 2) # outcome under treatment
Y0 <- L + rnorm(n = N, mean = 0) # outcome under no treatment

Y <- Y1*D + Y0*(1-D)

unadjusted_model <- lm(Y ~ D)

ggplot(data.frame(D = as.factor(D), Y = Y),
       aes(x = D, y = Y, color = D)) + 
  ggforce::geom_sina(alpha = 0.35, scale = 'width', maxwidth=.5) + 
  geom_boxplot(alpha = 0.5, width = .3, outlier.color = NA, color = 'black') + 
  stat_summary(fun = mean, geom = 'point', size = 3, shape = 1, color = 'black') + 
  theme_bw() + 
  theme(legend.position = 'bottom') + 
  ggtitle("D (treatment) and Y (outcome) are confounded by (unshown) L",
          "Averages are indicated by the open black circles") +
  labs(caption = paste0("The biased estimate of average treatment effect is roughly ", round(coef(unadjusted_model)[2], 1)))
```

```{r, out.width='6in', fig.width = 7.5, fig.align = 'center'}
ggplot(data.frame(L = L, D = as.factor(D), Y = Y),
       aes(x = L, y = Y, color = D)) + 
  geom_point(alpha = 0.25) + 
  geom_smooth(method = 'lm', se=F) + 
  theme_bw() + 
  theme(legend.position = 'bottom') + 
  labs(color = 'D (treatment):') + 
  scale_y_continuous(breaks = c(-4,-2,0,2,4,6)) + 
  ggtitle("Conditioning on L allows us to see that the average treatment effect is actually 2") 
```

So now can we check if the sharp bounds contain 2, which they should 
as long as the theorem holds? (check the book for the proof!)

```{r}
lower_bound <- mean(Y*D + min(Y)*(1-D) - Y*(1-D) - max(Y)*D)
upper_bound <- mean(Y*D + max(Y)*(1-D) - Y*(1-D) - min(Y)*D)

print("the true average treatment effect should be guaranteed to lie within:")
print(paste0("[", round(lower_bound, 1), ', ', round(upper_bound, 1), "]"))

print("is the actual average treatment effect within these bounds?")
if (2 >= lower_bound & 2 <= upper_bound) print("yup! üôÇ") else print("nope üòï")
```

## Inverse Probability Weighting

**Definition 6.2.8** Stabilized IPW Estimator for Missing Data

Let $Y_i$ and $R_i$ be random variables with 
$\text{support}(R_i) = \{ 0, 1 \}$. Let $Y_i^*$ be
as in the stable outcomes model and let $X_i$ be a random 
vector. Then given $n$ independently and identically distributed
observations of $(Y_i^*, R_i, X_i)$, the stabilized
IPW estimator for $\E[Y_i]$ is 

$$ \hat{\mathbb E}_{SIPW} (Y_i) = \frac{\frac{1}{n} \sum_{i=1}^n \frac{Y_i R^i}{\hat p_R(X_i)}}{\frac{1}{n} \sum_{i=1}^n \frac{R^i}{\hat p_R(X_i)}}. $$

