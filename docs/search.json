[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Readings in Causal Inference, Fall 2023",
    "section": "",
    "text": "During the Fall 2023 I’m doing a reading-based independent study and writing up my notes to share here from some classic and modern texts in causal inference with a particular interest on a few subtopics including 1) complex mediators, 2) the role of semiparametric methods, and 3) doubly-robust learning.\n\n\n\n\n\n\nReading List for the Semester\n\n\n\n\n\n\n\n\nArticle Reference\nLink to Article/Book\n\n\n\n\nRosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1), 41-55.\nArticle\n\n\nRobins, J. M., & Ritov, Y. (1997). Toward a curse of dimensionality appropriate (CODA) asymptotic theory for semi-parametric models. Statistics in medicine, 16(3), 285-319.\nArticle\n\n\nRobins, J. M., Hernán, M. Á., & Brumback, B. (2000). Marginal structural models and causal inference in epidemiology. Epidemiology, 11(5), 550-560.\nArticle\n\n\nDawid, A. P. (2000). Causal inference without counterfactuals. Journal of the American Statistical Association, 95(450), 407-424.\nArticle\n\n\nPearl, J. (2001). Direct and indirect effects. Proceedings of the seventeenth conference on uncertainty in artificial intelligence.\nArticle\n\n\nZhang, Z., & Rubin, D. B. (2003). Estimation of causal effects via principal stratification when some outcomes are truncated by ‘death’. Journal of Educational and Behavioral Statistics, 28(4), 353-368.\nArticle\n\n\nRubin, D. B. (2005). Causal inference using potential outcomes: Design, modeling, decisions. Journal of the American Statistical Association, 100(469), 322-331.\nArticle\n\n\nHernán, M. A., & Robins, J. M. (2006). Instruments for causal inference: an epidemiologist’s dream?. Epidemiology, 17(4), 360-372.\nArticle\n\n\nShalizi, C. R., & Thomas, A. C. (2010). Homophily and contagion are generically confounded in observational social network studies. Sociological methods & research, 40(2), 211-239.\nArticle\n\n\nVan der Laan, M. J., & Rose, S. (2011). Targeted Learning: Causal Inference for Observational and Experimental Data. Springer.\nBook\n\n\nPeng, R. D. (2011). Reproducible research in computational science. Science, 334(6060), 1226-1227.\nArticle\n\n\nTchetgen Tchetgen, E. J., & Shpitser, I. (2012). Semiparametric theory for causal mediation analysis: efficiency bounds, multiple robustness and sensitivity analysis. The Annals of Statistics, 40(3), 1816-1845.\nArticle\n\n\nPearl, J. (2012). The causal mediation formula—a guide to the assessment of pathways and mechanisms. Prevention science, 13(4), 426-436.\nArticle\n\n\nTchetgen Tchetgen, E. J. (2013). Identification and estimation of survivor average causal effects. Biometrika, 100(2), 503-518.\nArticle\n\n\nTchetgen Tchetgen, E. J. (2014). The control outcome calibration approach for causal inference with unobserved confounding. The American Statistician, 68(1), 27-32.\nArticle\n\n\nVanderWeele, T. J., & Tchetgen Tchetgen, E. J. (2014). Mediation analysis with time varying exposures and mediators. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(3), 523-545.\nArticle\n\n\nVanderWeele, T. J. (2015). Explanation in causal inference: methods for mediation and interaction. Oxford University Press.\nBook\n\n\nImbens, G. W., & Rubin, D. B. (2015). Causal Inference in Statistics, Social, and Biomedical Sciences. Cambridge University Press.\nBook\n\n\nPeters, J., Janzing, D., & Schölkopf, B. (2017). Elements of causal inference: foundations and learning algorithms. MIT press.\nBook\n\n\nRobins, J. M., Sued, M., Lei, G., & Mínguez, D. (2017). Comment: The self-controlled case series method - An innovative design for home and online randomized controlled trials. Statistical Science, 32(2), 264-267.\nArticle\n\n\nHernán, M. A., & Robins, J. M. (2018). Causal Inference. Chapman & Hall/CRC.\nBook\n\n\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21(1), C1-C68.\nLink\n\n\n\n\n\n\nSupplemental Articles\nLink to Article\n\n\n\n\nLu Cheng, Ruocheng Guo, and Huan Liu. 2022. Causal Mediation Analysis with Hidden Confounders. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM ’22). Association for Computing Machinery, New York, NY, USA.\nArticle\n\n\nZhang, Z., Zheng, W., & Albert, J. M. (2019). High-dimensional mediation analysis with latent variables in randomized clinical trials. Statistical Methods in Medical Research, 28(10-11), 3186-3200.\nArticle\n\n\nDerkach A, Pfeiffer RM, Chen TH, Sampson JN. High dimensional mediation analysis with latent variables. Biometrics. 2019 May 5.\nArticle\n\n\nZeng P, Shao Z, Zhou X. Statistical methods for mediation analysis in the era of high-throughput genomics: Current successes and future challenges. Computational and Structural Biotechnology Journal. 2021.\nArticle\n\n\nEric J. Tchetgen Tchetgen. Ilya Shpitser. “Semiparametric theory for causal mediation analysis: Efficiency bounds, multiple robustness and sensitivity analysis.” Ann. Statist. June 2012. https://doi.org/10.1214/12-AOS990\nArticle\n\n\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations. arXiv preprint arXiv:1802.05365.\nArticle\n\n\nNima S Hejazi and others, Nonparametric causal mediation analysis for stochastic interventional (in)direct effects, Biostatistics, July 2023. Article\nArticle\n\n\nHernán, M., Taubman, S. Does obesity shorten life? The importance of well-defined interventions to answer causal questions. Int J Obes 32 (Suppl 3), S8–S14 (2008). https://doi.org/10.1038/ijo.2008.82\nArticle\n\n\nPeters, J., Mooij, J. M., Janzing, D., & Schölkopf, B. (2014). Causal discovery with continuous additive noise models.\nArticle\n\n\nMooij, J. M., Peters, J., Janzing, D., Zscheischler, J., & Schölkopf, B. (2016). Distinguishing cause from effect using observational data: methods and benchmarks. The Journal of Machine Learning Research.\nArticle\n\n\n\n\n\n\n\nWeek 1: The Central Role of the Propensity Score in Observational Studies for Causal Effects by Paul R. Rosenbaum and Donald B. Rubin (1983), Biometrika\nHTML  PDF\n\nWeek 2a: Marginal Structural Models for Causal Inference by James M. Robin, Miguel Ángel Hernán, and Babette Brumback (2000), Epidemiology\nHTML  PDF\n\nWeek 2b: Causal Inference Without Counterfactuals  by Alexander Phillip Dawid (2000). JASA\nHTML PDF"
  },
  {
    "objectID": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#introduction",
    "href": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#introduction",
    "title": "3  Marginal Structural Models",
    "section": "Introduction",
    "text": "Introduction\nThe key problem this paper addresses is the bias induced by time-dependent confounders that are also affected by previous treatment — which are handled by the introduction of marginal structural models that can be consistently estimated using inverse-probability-of-treatment weighted (IPTW) estimators.\nDefinition. Time-dependent confounders are covariates that are a risk factor for, or predictor of, the event of interest and also predicts subsequent exposure.\nWe are particularly interested in time-dependent confounders that are also affected or predicted by past exposure history (Condition 2)."
  },
  {
    "objectID": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#time-dependent-confounding",
    "href": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#time-dependent-confounding",
    "title": "3  Marginal Structural Models",
    "section": "Time-Dependent Confounding",
    "text": "Time-Dependent Confounding\n\nConsider a follow-up study of HIV-infected patients. Let \\(A_k\\) be the dose of the treatment or exposure of interest, say zidovudine (AZT) on the \\(k\\)th day since start of follow-up. Let \\(Y\\) be a dichotomous outcome of interest (for example, \\(Y = 1\\) if HIV RNA is not detectable in the blood and 0 otherwise) measured at end of follow-up on day \\(K+1\\). Our goal is to estimate the time-dependent treatment \\(A_k\\) on the outcome \\(Y\\).\n\nLet \\(L_k\\) represent the vector of all measured risk factors on day \\(k\\) for the outcome such as age, CD4 lymphocyte count, white blood count, hematocrit, diagnosis of AIDS, and presence of absence of symptoms and opportunistic infections. Let \\(U_k\\) represent the value(s) on day \\(k\\) of all unmeasured causal risk factors for \\(Y\\).\n\n\n\n\n\nFigure 1a. The most complex of the causal graphs for time-dependent exposure.\n\n\n\n\n\n\n\n\n\nFigure 1b. Similar to the above graph, but missing the arrows from \\(U_{t_1}\\) to \\(A_{t_2}\\) for \\(t_1, t_2 \\in \\{0, 1\\}\\).\n\n\n\n\n\n\n\n\n\nFigure 1c. Again similar to the above graph, but also missing the arrows from \\(L_{t_1}\\) to \\(A_{t_2}\\) for \\(t_1, t_2 \\in \\{0, 1\\}\\).\n\n\n\n\nBefore diving immediately into how to address time-varying confounding that is affected by antecedent exposures, we establish some preliminary findings in the setting of point-treatment studies.\n\n\n\n\n\nFigure 2a. A causal graph for a point-exposure.\n\n\n\n\n\n\n\n\n\nFigure 2b. Similar to above but missing the arrow from \\(U_0 o A_0\\).\n\n\n\n\n\n\n\n\n\nFigure 2c. Again similar to above but missing the arrow from \\(L_0 o A_0\\).\n\n\n\n\n\nI think Robins, Hernán, and Brumback summarize the problem with time-varying confounders quite clearly in section 7.1:\n“… Standard regression methods adjust for covariates by including them in the model as regressors. These standard methods may fail to adjust appropriately for confounding due to measured confounders \\(L_k\\) when treatment is time varying because (1) \\(L_k\\) may be a confounder for later treatment and thus must be adjusted for, but (2) may also be affected by earlier treatment and thus should not be adjusted for by standard methods. A solution to this conundrum is to adjust for the time-dependent covariates \\(L_k\\) by using them to calculate the weights \\(sw_i\\) rather than by adding \\(L_k\\) to the regression model as regressors.”"
  },
  {
    "objectID": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#counterfactuals-in-point-treatment-studies",
    "href": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#counterfactuals-in-point-treatment-studies",
    "title": "3  Marginal Structural Models",
    "section": "Counterfactuals in Point-Treatment Studies",
    "text": "Counterfactuals in Point-Treatment Studies\nIn an effort to help orient the readers, the authors provide some basic background on causal inference in point-treatment studies (Figures 2a-2c) to help keep us all grounded before moving on to more complicated settings.\nThrough this section (\\(\\S2\\)) the authors contrast crude measures with causal measures explaining that the causal measures will equal the crude measures when the analysis is unconfounded.\n\\[cRD = Pr[Y = 1 | A_0 = 1] - Pr[Y = 1 | A_0 = 0] \\quad \\text{\\small (Crude Risk Difference)}\\] \\[cRR = Pr[Y = 1 | A_0 = 1]/Pr[Y = 1 | A_0 = 0] \\quad \\text{\\small (Crude Risk Ratio)}\\] \\[cOR = \\frac{Pr[Y = 1 | A_0 = 1]/Pr[Y=1 | A_0 = 0]}{Pr[Y=0 | A_0 = 1]/Pr[Y=0 | A_0 = 0]} \\quad \\text{\\small (Crude Odds Ratio)}\\]\n\\[Pr[Y_{a_0 = 1} = 1] - Pr[Y_{a_0 = 0} = 1] \\quad \\text{\\small (Causal Risk Difference)}\\] \\[Pr[Y_{a_0 = 1} = 1]/Pr[Y_{a_0 = 0} = 1] \\quad \\text{\\small (Causal Risk Ratio)}\\] \\[\\frac{Pr[Y_{a_0 = 1} = 1]/Pr[Y_{a_0 = 0} = 1]}{Pr[Y_{a_0=1}=0]/Pr[Y_{a_0=0} = 0]} \\quad \\text{\\small (Causal Risk Ratio)}\\]\nNote that the way I’ve written the odds ratios is to make the structure of them obvious as ratios of odds, but of course one can re-express any fraction written \\(\\displaystyle \\frac{a / b}{c / d} = \\frac{a d}{b c}\\), which is how they’re presented in the paper.\nAn important point they make is that because of the possibility of effect modification, the population causal parameter need not be the same as its estimate in a particular stratum of measured risk factors even if the treatment is unconfounded.\nTo estimate these quantities of interest, we might fit linear, exponential, and logistic models depending on expert knowledge about the nature of the data being considered (and what type of exposure-response curve we expect):\n\\[Pr[Y_{a_0} = 1] = \\phi_0 + \\phi_1 a_0\\] \\[\\log Pr[Y_{a_0} = 1] = \\theta_0 + \\theta_1 a_0\\] \\[\\text{logit} Pr[Y_{a_0} = 1] = \\beta_0 + \\beta_1 a_0\\]\nInterpreting, the causal RD is \\(\\phi_1\\), the causal RR is \\(e^{\\theta_1}\\), and the causal OR is \\(e^{\\beta_1}\\).\nThese models are described as marginal because they model the marginal distribution of the counterfactual random variables \\(Y_{a_0=1}\\) and \\(Y_{a_0=1}\\) rather than the joint distribution (as in, they do not model \\(\\text{cor}(Y_{a_0=1}, Y_{a_0=0})\\)). They are said to be structural because they model the probabilities of counterfactual variables (apparently models for counterfactual variables are often called structural in the econometrics and social science literature).\nI’m a little confused about their claim that these are saturated models: they say that they’re saturated because each model has two unknown parameters and places no restriction on the probabilities for \\(Y_{a_0 = 1}\\) and \\(Y_{a_0 = 0}\\). I guess I’m just used to a different definition of saturated in which saturation refers to the model having as many parameters as it has data points, which does not feel like it’s necessarily true here."
  },
  {
    "objectID": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#no-unmeasured-confounders",
    "href": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#no-unmeasured-confounders",
    "title": "3  Marginal Structural Models",
    "section": "No Unmeasured Confounders",
    "text": "No Unmeasured Confounders\nNext a claim is introduced: if we weight the observations by \\(w_i = 1/Pr[A_0 = a_{0i} | L_0 = l_{0i}]\\) where \\(l_{0i}\\) and \\(a_{0i}\\) are the measured covariates and exposures for subject \\(i\\), then we should recover unbiased estimates of the causal quantities of interest.\nA rough sketch of why this should work is presented that rests on two additional claims:\n\nIn the constructed pseudopopulation, \\(A_0\\) should be unconfounded by the measured covariates \\(L_0\\).\n\\(Pr(Y_{a_0=1} = 1)\\) and \\(Pr(Y_{a_0=1} = 0)\\) are the same as in the true study population so that the causal RD, RR, and OR are the same in both populations.\n\nProof of 1. We want to show that \\[Pr^W(A = a | L = l) \\stackrel{claim}{=} Pr^W(A = a),\\] where \\(Pr^W\\) refers to probability in the reweighted pseudopopulation.\nI will only show this for the case where both \\(A\\) and \\(L\\) are dichotomous.\nFirst, we can establish that \\[Pr^W(A = a | L = l) = \\frac{N_{(A = a, L =l)} w}{\\sum_{i \\colon L_i = l} w_i},\\] where \\(w = 1/Pr(A = a|L=l)\\) is a scalar-value and\n\\[w_i = \\left\\{ \\begin{array}{ll} 1/Pr(A=1|L=l) & \\text{ if } A_i = 1 \\\\\n1/Pr(A=0|L=l) & \\text{ if } A_i = 0 \\end{array}\\right.\\]\nwhere \\(N_{(A=a,L=l)} \\stackrel{\\text{def}}{=} \\sum_{i \\colon a_i = a, l_i = l} 1\\).\nLetting \\(w' = 1/Pr(A = 1-a | L=l)\\), then we have that:\n\\[Pr^W(A = a | L = l) = \\frac{N_{(A=a, L=l)}w}{N_{(A=a,L=l)}w + N_{(A=1-a,L=l)}w'}.\\]\nSubstituting that \\(Pr(A = a | L = l)\\) is estimated by \\(N_{(A=a,L=l)}/N_{(L=l)}\\), we have that\n\n\\[Pr^W(A = a | L = l) = \\frac{\\cancel{N_{(A=a, L=l)}} \\frac{N_{(L=l)}}{\\cancel{N_{(A=a,L=l)}}}}{\n\\cancel{N_{(A=a,L=l)}} \\frac{N_{(L=l)}}{\\cancel{N_{(A=a,L=l)}}} + \\cancel{N_{(A=1-a,L=l)}} \\frac{N_{(L=l)}}{\\cancel{N_{(A=1-a,L=l)}}}}\\] \\[ = \\frac{N_{(L=l)}}{2 N_{(L=l)}}\\] \\[ = \\frac{1}{2}\\]\nIn general, if \\(L\\) took on more than just two values, we find that the probability that \\(A=a\\) is the inverse of the number of levels of \\(L\\), which is independent of any single value that \\(L\\) could take on.\n\n\\(\\square\\)\n\nIf we consider the 3-way tables necessary in the situation when \\(A\\), \\(L\\), and \\(Y\\) are each dichotomous, we’re looking at something like this (for a simulated study of 1,000 participants):\n\n\n\n\n\n\n\n\n\nAfter applying inverse probability of treatment weights:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter some searching, I believe I have found a proof, in Rebecca Barter’s blog-post on The intuition behind inverse probability weighting in causal inference.\nBefore getting into the proof, she helpfully reminds us that a causal estimand is identifiable when (1) exchangeability/ignorability, (2) consistency, and (3) positivity all hold.\nDefinition. Exchangeability is the assumption we can re-arrange our observations without altering our conclusions. In the context of causal inference, we’re saying that we’d like our observations to be identically and independently distributed, same for the treatment assignment mechanism, and that therefore except for the treatment individual units received, we can permute them without changing our results.\nDefinition. Consistency refers to whether or not our model of potential (unobserved) outcomes is valid, in that our model is consistent with the reality of the data generating processes if we are making accurate statements regarding \\(Pr(\\text{outcomes}|\\text{exposures}).\\)\nDefinition. Positivity refers to the fact that all observations necessarily must have strictly positive probability of being assigned to either the treatment or control group — because otherwise if there is 0 probability of assignment to one of the treatment or control groups, we will necessarily lack observations from that group and thus unable to form causal estimates.\nIf we were to have exchangeability, consistency, and positivity, then we could say that the average treatment effect is estimated by \\[\\frac{\\sum_{i \\colon A=1} Y_i}{N_{(A=1)}} - \\frac{\\sum_{i \\colon A = 0} Y_i}{N_{(A=0)}},\\] since by exchangeability the observations are not confounded.\nNow we can proceed to proving the claim that re-weighting the population as described produces a pseudopopulation in which the causal estimate is unbiased.\nProof. Let \\(p(l) = Pr(A=1|L=l)\\) where for treated individuals we will weight them by \\(1/p(l)\\) and for untreated individuals we will weight them by \\(1/(1-p(l))\\) and we would estimate \\(\\hat p\\) by a logistic regression model.\nThe causal estimate with inverse probability of treatment weighting is then given by:\n\\[\\sum_{i \\colon A=1} \\frac{Y_i}{N_{(A=1)} \\hat p(L_i)} - \\sum_{i \\colon A = 0}\n\\frac{Y_i}{N_{(A=0)} (1-\\hat p(L_i))}\\]\n\\[ = \\sum_{i}^n \\frac{Y_iA_i}{n \\hat p(L_i)} - \\sum_{i}^n\n\\frac{Y_i(1-A_i)}{n (1-\\hat p(L_i))},\\]\nwhere \\(n\\) is the total population size of observations and \\(N_(...)\\) is my short-hand notation for \\(\\displaystyle \\sum_{i \\colon ...} 1\\).\nWhat remains to be checked is if the above is equal to \\(\\mathbb E[Y_{A=1}] - \\mathbb E[Y_{A=0}]\\) (with respect to the super-population that we assume is exchangeable/unconfounded).\nWe will do so by verifying that \\(\\mathbb E\\left[ \\frac{YA}{p(L)} \\right] = \\mathbb E[Y_{A=1}]\\) (and the same steps would follow for showing \\(\\mathbb E\\left[ \\frac{Y(1-A)}{1-p(L)} \\right] = \\mathbb E[Y_{A=0}]\\).\n\\[\\begin{aligned}\n\\mathbb E\\left[ \\frac{YA}{p(L)} \\right] & = \\mathbb E\\left[\\mathbb E\\left[ \\frac{YA}{p(L)} \\middle\\vert L \\right] \\right] \\\\\n& = \\mathbb E\\left[\\mathbb E\\left[ \\frac{Y_{A=1}A}{p(L)} \\middle\\vert L \\right] \\right] \\\\\n& = \\mathbb E\\left[\\frac{\\mathbb E[Y_{A=1}|L] \\mathbb E[A|L]}{p(L)} \\right] \\\\\n& = \\mathbb E\\left[\\frac{\\mathbb E[Y_{A=1}|L] \\cancel{\\mathbb E[A|L]}}{\\cancel{\\mathbb E[A|L]}} \\right] \\\\\n& = \\mathbb E\\left[\\mathbb E[Y_{A=1}|L]\\right] \\\\\n& = \\mathbb E\\left[Y_{A=1}\\right]. \\\\\n\\end{aligned}\n\\]\nHence we can conclude that the inverse probability of treatment weighting yields an unbiased estimator for the causal effect.\n\n\\(\\square\\)"
  },
  {
    "objectID": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#stabilized-weights",
    "href": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#stabilized-weights",
    "title": "3  Marginal Structural Models",
    "section": "Stabilized Weights",
    "text": "Stabilized Weights\nRobins, Hernán, and Brumback go on to say that if levels of \\(A\\) and \\(L\\) are strongly associated, then it is likely that we will see quite large inverse probability of treatment weights. Such undesired variability in our estimated weights suggest that few individuals may inappropriately dominate the pseudopopulation and hence the weighted analysis.\nTo mitigate this, they introduce “stabilized weights” which are written as \\[sw_i = \\frac{Pr(A = a_i)}{Pr(A=a_i | L=l_i)},\\] which are the same weights as before just multiplied by \\(Pr(A=A_i)\\).\nI am suspicious that the reason we’re able to do this trick is because if we refer back to the proof that the original weights provided an unbiased estimator, we saw that \\[\\mathbb E\\left[ \\frac{Y_{A=1}A}{p(L)} \\right] = \\mathbb E\\left[ Y_{A=1} \\right],\\] and if we work the same argument through with the stabilized weights, I think we’d get \\(\\mathbb E\\left[ Y_{A=1} \\mathbb E[A] \\right]\\) instead. However, since \\(\\mathbb E[Y_{A=1}]\\) is the expected value of \\(Y\\) when \\(A=1\\), I think we can say that the prior quantity is equal to \\(\\mathbb E\\left[ Y \\mathbb E[A] \\middle\\vert A = 1\\right] = \\mathbb E\\left[ Y \\middle \\vert A = 1 \\right] = \\mathbb E[ Y_{A=1} ].\\)"
  },
  {
    "objectID": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#time-dependent-treatments",
    "href": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#time-dependent-treatments",
    "title": "3  Marginal Structural Models",
    "section": "Time-Dependent Treatments",
    "text": "Time-Dependent Treatments\nReturning to the setting of time-dependent confounding as in Figures 1a-c, let \\(A_k\\) represent the dosage of a treatment on the \\(k\\)th day from start of follow-up, \\(Y\\) is a dichotomous variable measured at the end of follow-up on day \\(K+1\\) , and \\(L_k\\) represents all measured risk factors for the outcome on day \\(k\\). We’ll let \\(\\bar A_k = (A_0, A_1, ..., A_k)\\) be the treatment or exposure history thorugh day \\(k\\) and let \\(\\bar A = \\bar A_K\\).\nGenerally we’ll be more interested in the cumulative effect of dosage (\\(\\sum a_k\\)) rather than specific dosage histories \\(\\bar a\\) when thinking about counterfactuals.\nAssuming no loss to follow-up selection bias or measurement error, we can unbiasedly estimate causal quantities using a logistic regression model\n\\[ \\text{logistic}(Pr[Y=1 | \\bar A = \\bar a]) = \\beta_0 + \\beta_1 \\sum a_k, \\] if treatment is unconfounded.\nHowever, if treatment is confounded, we need to reweight the population by stabilized weights given by\n\\[sw_i = \\frac{\\displaystyle \\prod_{k=0}^K Pr(A_k = a_{ki} | \\bar A_{k-1} = \\bar a_{(k-1)i})}{\n\\displaystyle \\prod_{k=0}^K Pr(A_k = a_{ki} | \\bar A_{k-1} = \\bar a_{(k-1)i}, \\bar L_k = \\bar l_{ki}).\n}\\]"
  },
  {
    "objectID": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#censoring-by-loss-to-follow-up",
    "href": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#censoring-by-loss-to-follow-up",
    "title": "3  Marginal Structural Models",
    "section": "Censoring by Loss to Follow-Up",
    "text": "Censoring by Loss to Follow-Up\nThey say that we can also consider censoring at time \\(k\\) (denoted \\(C_k\\)) as another aspect of treatment and re-work our analyses to become “inverse-probability-of-treatment-and-censoring weighted estimators” if we view \\((A_k, C_k)\\) as a joint-treatment at time \\(k\\)."
  },
  {
    "objectID": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#limitations",
    "href": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#limitations",
    "title": "3  Marginal Structural Models",
    "section": "Limitations",
    "text": "Limitations\nThe authors state that marginal structural models cannot be used to model the interaction of treatment with a time-varying covariate (so effect modification). For this, they recommend structural nested models to be used.\nAs suggested above, it’s also the case that we cannot use marginal structural models when we have violations to the positivity assumption."
  },
  {
    "objectID": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#important-points-from-the-appendix",
    "href": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#important-points-from-the-appendix",
    "title": "3  Marginal Structural Models",
    "section": "Important Points from the Appendix",
    "text": "Important Points from the Appendix\nIf we recall the recommendations from Rosenbaum and Rubin, 1983, it’s worth distinguishing the approach suggested here from the propensity score based approaches previously. The propensity score methods were largely oriented towards either propensity score stratification, propensity score matching, or covariance adjustment for propensity scores. It’s very much worth noting that the inverse of the propensity score is not the same as the inverse probability of treatment proposed here, as the propensity score always models \\(Pr(A=1|L=l)\\) while the probability of treatment is the probability of the treatment a specific unit received, so \\(Pr(A=a_i | L=l_i)\\). In other words, for those units which were treated, the propensity score and the probability of treatment agree, but for untreated units, the probability of the treatment they received (no-treatment) is \\(1-Pr(A=1|L=l_i)\\).\nAs an additional consideration, it’s worth noting that matching or stratification methods could introduce significant bias if there is difficulty finding sufficiently close matches or residual uncontrolled-for intrastratum confounding."
  },
  {
    "objectID": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#introduction",
    "href": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#introduction",
    "title": "2  The Central Role of Propensity Scores",
    "section": "Introduction",
    "text": "Introduction\nThis paper introduces the reader to the propensity score and shows how, through large and small sample theory, that adjustment for the scalar propensity score is sufficient to remove bias due to the observed covariates.\nThe quantity of fundamental interest is the average treatment effect (presented in their notation):\n\\[\\mathbb E(r_1) - \\mathbb E(r_0),\\] where \\(r_1\\) is the outcome under treatment and \\(r_0\\) is the outcome under no treatment.\nThey introduce balancing scores which are functions \\(b(x)\\) such that \\[x \\perp\\!\\!\\!\\perp z | b(x) \\quad (\\text{or equivalently} \\quad  z \\perp\\!\\!\\!\\perp x | b(x)).\\]\nIn other words, \\(P(Z|X,b(X)) = P(Z|b(X))\\).\nWe define the propensity score to be \\[e(x) = Pr(Z=1|X = x)\\] e.g., the probability of treatment given covariates}, is the propensity score or propensity towards treatment."
  },
  {
    "objectID": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#an-example",
    "href": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#an-example",
    "title": "2  The Central Role of Propensity Scores",
    "section": "An Example",
    "text": "An Example\nWe might have a situation where age is correlated with assignment to the treatment mechanism.\nDenoting \\(X\\) as age and \\(Z\\) as treatment, consider the following scenario where we set\n\\[X \\sim \\text{Uniform on } \\mathbb N \\cap \\text{ the age range } [19,64]\\] \\[Z \\sim \\text{Bernoulli}(\\underbrace{\\text{logistic}(\\underbrace{\\beta_0 + \\beta_1 \\cdot X}_{\\text{log odds}})}_{\\text{probability scale}})\\] \\[ \\text{where logistic}(x) = \\frac{1}{1+e^{-x}} = \\frac{e^x}{1+e^x}\\]\nIt’s clear that \\(X\\) and \\(Z\\) will be dependent and correlated, but by stratifying on propensity scores estimated via a logistic regression model, we can empirically see an example of\nhow conditioning on propensity scores renders \\(X\\) and \\(Z\\) conditionally independent.\n\n# Dependencies\nlibrary(tidyverse)\n\n# Generate a data.frame of 100 folks with ages from 19-65\ndf <- data.frame(x = sample(x=seq(19,65), replace=TRUE, size = 100))\n\n# Coefficients for our logistic model\nbeta_0 <- -5\nbeta_1 <- 0.1\n\n# Compute the odds of treatment using a logistic function\ndf$e_x <- exp(beta_0 + beta_1 * df$x) / (1 + exp(beta_0 + beta_1 * df$x))\n\n# Assign treatment group based on the computed odds\ndf$z <- rbinom(n = nrow(df), size = 1, prob = df$e_x)\n\n# Fit a logistic regression model \nmodel <- glm(z ~ x, data = df, family = binomial(link=logit))\n\n# Report model estimates\ncoef(model)\n\n# Predicted propensity scores\ndf$propensity_scores <- predict(model, type=\"response\")\n\n# We can confirm these match our intuition — \n# This is just applying the logistic function 1/(1+exp(-x)) to the\n# expanded terms from our regression model.\nall.equal(df$propensity_scores, 1/(1+exp(-(coef(model)[1] + coef(model)[2]*df$x))))\n\n# Plot our age-treatment relationship\nggplot(df, aes(x = x, y = z)) + \n  geom_jitter(width=0.25, height=0.025, alpha = 0.5) + \n  geom_line(data = \n    data.frame(x = seq(18,65,.1), \n      y = predict(model, newdata = data.frame(x = seq(18,65,.1)), type='response')),\n    aes(x = x, y = y),\n    color = 'blue') + \n  xlab(\"Age\") + \n  ylab(\"Treatment, or Treatment Probability\") + \n  ggtitle(\"Distribution of Treatment Assignment by Age and Logistic Model\") + \n  theme_bw()\n\n\n# Divide the data into bins based on the propensity score\ndf$e_bin <- cut(df$e_x, breaks = seq(0, 1, by = 0.1))\n\n# Calculate the mean age by propensity score in\nmean_x_by_bin <- df |> group_by(e_bin, z) |> summarize(x = mean(x))\n\n# Show age by propensity score bins and treatment\nggplot(df, aes(x = e_bin, y = x, color = as.factor(z))) +\n  geom_jitter(width=.25, height = .05, alpha = 0.5, size = 1) +\n  geom_line(data = mean_x_by_bin, aes(group = z)) +\n  labs(title = \n    \"Balance Check: Mean Age (Line) by Propensity Score Bin and Treatment Status\"),\n       x = \"Propensity Score Bin\",\n       y = \"Age\",\n       color = \"Treatment Status\")\n\n\n\n\n\n\nThe distribution of treatment assignment by age and a fit logistic model.\n\n\n\n\n\n\n\n\n\nA balance check showing that after stratifying by the propensity scores, age and assignment to treatment are independent."
  },
  {
    "objectID": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#theorem-1",
    "href": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#theorem-1",
    "title": "2  The Central Role of Propensity Scores",
    "section": "Theorem 1",
    "text": "Theorem 1\n\\(e(X)\\) is a balancing score. That is, \\[X \\perp\\!\\!\\!\\perp Z | e(X).\\]\nProof. We want to show that \\[Pr(Z = 1 | X, e(X)) = Pr(Z = 1 | e(X)).\\]\nSince \\(e(x)\\) is a function of \\(x\\), we have that \\[Pr(Z = 1 | X, e(X)) = Pr(Z = 1 | X = X) = e(X).\\]\nWhat remains is to show that \\(Pr(Z = 1 | e(X)) = e(X)\\).\nBy the definition of probability and the law of iterated expectation, \\[\\begin{aligned}\nPr(Z = 1 | e(X)) & = \\mathbb E[Z = 1 | e(X)] \\quad \\text{ (by definition of probability) } \\\\\n& = \\mathbb E[\\mathbb E(Z = 1|X) | e(X)] \\quad (\\star) \\\\\n& = \\mathbb E[ e(X) | e(X) ] \\\\\n& = e(x) \\quad \\text{(by the law of iterated expectations)}\n\\end{aligned}\\]\nThe \\((\\star)\\) step can be justified two ways, either rhetorically or more rigorously:\n\nRhetorically: With respect to the inside term, we can apply extra conditioning by \\(X = x\\) since we’re going to compute the outer expectations conditioned on \\(e(X)\\) and thus across the \\(X\\) values that satisfy \\(X = x \\in e^{-1}(x)\\).\nMore rigorously: \\[ \\begin{aligned}\n    \\mathbb E[Z = 1 | e(X')] & = \\mathbb E[Z = 1 | X \\in e^{-1}(X')] \\\\\n    & = \\frac{1}{Pr(X \\in e^{-1}(X'))} \\int z Pr(Z = 1, X \\in e^{-1}(X')) dz \\\\\n    & = \\frac{1}{Pr(X \\in e^{-1}(X'))} \\int z \\int_{x \\in e^{-1}(X')} Pr(Z = 1 | X = x) dx dz \\\\\n    & = \\mathbb E[\\mathbb E[Z = 1 | X ] | X \\in e^{-1}(X')] \\\\\n    & = \\mathbb E[\\mathbb E[Z = 1 | X ] | e(X')] \\\\\n    & = \\text{(continue from above)}\n    \\end{aligned}\\]\n\nThus \\[Pr(Z = 1 | X, e(X)) = e(X) = Pr(Z = 1 | e(X)).\\]\n\n\\(\\square\\)"
  },
  {
    "objectID": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#theorem-2",
    "href": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#theorem-2",
    "title": "2  The Central Role of Propensity Scores",
    "section": "Theorem 2",
    "text": "Theorem 2\nLet \\(b(x)\\) be a function of \\(x\\). Then \\(b(x)\\) is a balancing score, that is \\[x \\perp\\!\\!\\!\\perp z | b(x)\\] if and only if \\(b(x)\\) is finer than \\(e(x)\\) in the sense that \\(e(x) = f(b(x))\\) for some function \\(f\\).\nProof. (\\(\\leftarrow\\)) First we show that if \\(b(x)\\) is finer than \\(e(x)\\) then it is a balancing score.\nWe want to show that \\[ Pr(Z = 1 | X, b(X)) \\stackrel{\\text{claim}}{=} Pr(Z = 1 | b(X))\\]\nSince the left hand side is easily understood to be \\(e(x)\\) since \\(b(X)\\) is a function of \\(X\\), it is then sufficient to show that \\[Pr(Z=1|b(x)) = e(x).\\]\nNow we rewrite the left-hand-side as follows: \\[Pr(Z=1|b(x)) = \\mathbb E[Z=1|b(X)] = \\mathbb E[\\mathbb E[Z=1|X]|b(X)] = \\mathbb E[e(x)|b(x)].\\]\nThe next step of Rosenbaum and Rubin’s is to claim that \\[\\mathbb E[e(X) | b(X) = b(x)] = e(x).\\]\nThe most straightforward way to see this is to start by noting that if we fix \\(X=x\\), then any function of \\(X\\) as a random variable also becomes fixed, so that \\(\\mathbb E[f(X)|X=x] = f(x)\\). In our case, we have that \\(e(x) = f(b(x))\\), so \\(\\mathbb E[e(x)|b(x)] = \\mathbb E[f(b(X))|b(X)=b(x)] = f(b(x)) = e(x)\\).\nAs a result, we’ve shown that \\[\\mathbb E[e(x)|b(x)] = Pr(Z = 1|b(x)) = e(x),\\] which concludes this direction of the proof showing that if \\(b(x)\\) is finer than \\(e(x)\\) then it is a balancing score.\n\\[ \\exists f \\colon e(x) = f(b(x)) \\Longrightarrow X \\perp\\!\\!\\!\\perp Z | b(x).\\]\n(\\(\\rightarrow\\)) Moving on, we now prove the converse direction by contradiction. Suppose that we have \\(b(x)\\), a balancing score such that \\(X \\perp\\!\\!\\!\\perp Z | b(x)\\), but \\(b(x)\\) is not finer than \\(e(x)\\) so we have \\(\\exists x_1, x_2 \\ni e(x_1) \\neq e(x_2) \\text{ and } b(x_1) = b(x_2)\\).\nWe know by applying the definition of \\(e(\\cdot)\\) that \\[Pr(Z = 1 | X = x_1) \\neq Pr(Z = 1 | X = x_2).\\] However, if \\(X \\perp\\!\\!\\!\\perp Z | b(x)\\), then we should have that\n\\[Pr(Z = 1 | X = x_1, b(x_1)) = Pr(Z=1 | b(x_1)), \\quad \\text{ and }\\] \\[Pr(Z = 1 | X = x_2, b(x_2)) = Pr(Z=1 | b(x_2)).\\]\nSince \\(b(x_1) = b(x_2)\\), we also then know that \\[Pr(Z=1 | b(x_1)) = Pr(Z=1 | b(x_2)),\\] but this would imply that \\[e(x_1) = Pr(Z = 1 | X = x_1, b(x_1)) = Pr(Z = 1 | X = x_2, b(x_2)) = e(x_2),\\] since \\(Pr(Z|X,f(X)) = Pr(Z|X)\\) for any function \\(f\\). This establishes a contradiction to our assumption that \\(e(x_1) \\neq e(x_2)\\).\nThus if \\(b(x)\\) is a balancing function then it must be finer than \\(e(x)\\). \\[X \\perp\\!\\!\\!\\perp Z | b(x) \\Longrightarrow e(x) = f(b(x)).\\]\n\n\\(\\square\\)"
  },
  {
    "objectID": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#section",
    "href": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#section",
    "title": "2  The Central Role of Propensity Scores",
    "section": "",
    "text": "Definition. We say that treatment assignment is strongly ignorable given a vector of covariates \\(v\\) if and only if \\[(r_1,r_0) \\perp\\!\\!\\!\\perp z | v, \\quad 0 < Pr(Z=1|v) < 1.\\]"
  },
  {
    "objectID": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#theorem-3",
    "href": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#theorem-3",
    "title": "2  The Central Role of Propensity Scores",
    "section": "Theorem 3",
    "text": "Theorem 3\nIf treatment assignment is strongly ignorable given \\(x\\), then it is strongly ignorable given any balancing score \\(b(x)\\); that is, \\[(r_1, r_0) \\perp\\!\\!\\!\\perp Z | X\\] and \\[0 < Pr(Z=1 | X) < 1\\] for all \\(x\\) imply \\[(r_1, r_0) \\perp\\!\\!\\!\\perp Z | b(X)\\] and \\[0 < Pr(Z = 1 | b(X)) < 1\\] for all \\(b(X)\\).\nProof. It is clear that if \\(0 < Pr(Z = 1|x) < 1\\) for all values of \\(x\\), then there is no such value of \\(x\\) where conditioning on \\(b(x)\\) would push the quantity \\(Pr(Z=1|b(x))\\) outside the interval \\((0,1)\\).\nWhat remains then is to show that \\((r_1, r_0) \\perp\\!\\!\\!\\perp z | b(x)\\), or rewritten that \\[Pr(Z=1 | r_1, r_0, b(x)) = Pr(Z=1 | b(x)),\\] which, if we recall the proof to Theorem 2, we have shown \\[ = e(x).\\]\nIn order to show the claimed equality, we rewrite the probability of treatment given \\(r_1, r_0,\\) and \\(b(x)\\) as the expected value of \\(Pr(Z=1 | r_1, r_0, X)\\) conditioned on \\(b(x)\\). We then have, by the assumption that \\((r_1, r_0) \\perp\\!\\!\\!\\perp Z | X\\),\n\\[\\begin{aligned}\nPr(Z = 1 | r_1, r_0, b(x)) & = \\mathbb E[Pr(Z=1|r_1,r_0,X)|b(x)] \\\\\n& = \\mathbb E[Pr(Z=1|X)|b(x)] \\\\\n& = \\mathbb E[e(X)|b(x)] = e(x).\n\\end{aligned}\\]\n\n\\(\\square\\)"
  },
  {
    "objectID": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#theorem-4",
    "href": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#theorem-4",
    "title": "2  The Central Role of Propensity Scores",
    "section": "Theorem 4",
    "text": "Theorem 4\nSuppose treatment assignment is strongly ignorable and \\(b(x)\\) is a balancing score. Then the expected difference in observed responses to the two treatments at \\(b(x)\\) is equal to the average treatment effect at \\(b(x)\\), that is,\n\\[\\mathbb E[r_1 | b(x), Z= 1] - \\mathbb E[r_0 | b(x), Z= 0] = \\mathbb E[r_1 - r_0 | b(x)].\\]\nProof. If a randomly selected treated unit (\\(Z = 1\\)) is compared to a randomly selected control unit (\\(Z = 0\\)), the expected difference in outcome is \\[\\mathbb E[r_1 | Z = 1] - \\mathbb E[r_0 | Z=0].\\]\nThis expression does not equal \\(\\mathbb E[r_1] - \\mathbb E[r_0]\\) in general because we do not observe \\(r_1\\) for untreated units and vice-versa and instead only observe the conditional distribution of \\(r_t\\) given \\(Z = t\\).\nWe introduce a two-stage sampling procedure now, where first we sample a vector of covariates \\(x\\) and then sample both treated and control units that have covariates equal to \\(x\\). The expected difference is now \\[\\mathbb E_{x} [ E[r_1 | X=x, Z=1] - \\mathbb E[r_0 | X=x, Z=0]],\\] where \\(\\mathbb E_{x}\\) denotes expectation with respect to the distribution of covariates \\(X\\). If treatment assignment is strongly ignorable, then the above is equal to \\[ \\mathbb E_x [E[r_1|x] - E[r_0|x]],\\] which is the average treatment effect.\nNow suppose we alter the two-stage sampling procedure to sample a value of \\(b(x)\\) instead of a vector of covariate levels. Given strongly ignorable treatment assignment, it follows from Theorem 3 that\n\\[\\mathbb E[r_1 | b(x), Z = 1] - \\mathbb E[r_0 | b(x), Z=0] = \\mathbb E[r_1 | b(x)] - \\mathbb E[r_0 | b(x)].\\]\nFrom this it follows that\n\\[\\begin{aligned} \\mathbb E_{b(x)} [ \\mathbb E[r_1| b(x), Z=1] - \\mathbb E[r_0 | b(x), Z = 0]] &\n= \\mathbb E_{b(x)} [ \\mathbb E[r_1 | b(x)] - \\mathbb E[r_0 | b(x)]] \\\\\n& = \\mathbb E[r_1 - r_0]. \\end{aligned}\\]\nTo quote:\n\nIn words, under strongly ignorable treatment assignment, units with the same value of the balancing score \\(b(x)\\) but different treatments can act as controls for each other, in the sense that the expected difference in their responses equals the average treatment effect.\n\n\n\\(\\square\\)"
  },
  {
    "objectID": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#corollaries",
    "href": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#corollaries",
    "title": "2  The Central Role of Propensity Scores",
    "section": "Corollaries",
    "text": "Corollaries\nParaphrasing: These theorems provide a rigorous justification for the estimation of average treatment effects via either pair matching on balancing scores or by estimating those treatment effects within strata of equal (or similar) balancing scores.\n\nCorollary 4.3. Covariance adjustment on balancing scores. Suppose treatment assignment is strongly ignorable, so that in particular \\(\\mathbb E[r_t|Z = t, b(x)] = \\mathbb E[r_t|b(x)]\\) for balancing score \\(b(x)\\). Further suppose that the conditional expectation of \\(r_t\\) given \\(b(x)\\) is linear: \\[\\mathbb E[r_t | Z=t,b(x)] = \\alpha_t + \\beta_t b(x) \\quad (t=0,1).\\] Then the estimator \\[(\\hat \\alpha_1 - \\hat \\alpha_0) + (\\hat \\beta_1 - \\hat \\beta_0)b(x)\\] is conditionally unbiased given \\(b(x_i)\\) for the treatment effect at \\(b(x)\\), namely \\(\\mathbb E[r_1-r_0|b(x)]\\), if \\(\\hat \\alpha_t\\) and \\(\\hat \\beta_t\\) are conditionally unbiased estimators of \\(\\alpha_t\\) and \\(\\beta_t\\) such as least squares estimators. Moreover \\[(\\hat \\alpha_1 - \\hat \\alpha_0) + (\\hat \\beta_1 - \\hat \\beta_0) \\bar b,\\] where \\(\\bar b = n^{-1} \\sum b(x_i)\\) is unbiased for the average treatment effect if the units in the study are a simple random sample from the population.\n\nCorollary 4.3 feels particularly different from corollary 4.1 and 4.2 (which are only summarized above) to me in that the above simulation (to me) clearly demonstrates how pair-matching and stratification on balancing scores would work. However, the covariate adjustment proposed in corollary 4.3 feels like something else, so I thought having some simulation code to demonstrate it made sense.\n\n# 1. Generate data\nn <- 200\nx <- rnorm(n)\nb_x <- 1 / (1 + exp(-x))  # a simple logistic propensity score\n\n# 2. Simulate treatment assignments\nz <- rbinom(n, 1, b_x)\n\n# 3. Simulate potential outcomes\nalpha_0 <- 2\nalpha_1 <- 3\nbeta_0 <- 1.5\nbeta_1 <- 2\nr_0 <- alpha_0 + beta_0 * b_x + rnorm(n)\nr_1 <- alpha_1 + beta_1 * b_x + rnorm(n)\n\n# Observed outcomes\nr <- z * r_1 + (1 - z) * r_0\ndf <- data.frame(b_x = b_x, z = z, r = r)\n\n# 4. Use linear regression to estimate parameters\nmodel_0 <- lm(r ~ b_x, data = df[z == 0,])\nmodel_1 <- lm(r ~ b_x, data = df[z == 1,])\n\nhat_alpha_0 <- coef(model_0)[1]\nhat_alpha_1 <- coef(model_1)[1]\nhat_beta_0 <- coef(model_0)[2]\nhat_beta_1 <- coef(model_1)[2]\n\n# 5. Compute the estimator\nb_bar <- mean(b_x)\nATE_est <- (hat_alpha_1 - hat_alpha_0) + (hat_beta_1 - hat_beta_0) * b_bar\n\nprint(ATE_est)\n\n(Intercept) \n   1.438144 \n\n# 6. Compare to the true average treatment effect\ntrue_ATE <- (alpha_1 - alpha_0) + (beta_1 - beta_0) * b_bar\nprint(true_ATE)\n\n[1] 1.239088\n\n# 7. Bootstrap to estimate SE of ATE_est\n\nB <- 1000  # number of bootstrap samples\nATE_boot <- numeric(B)\n\nfor (i in 1:B) {\n  # Resample data with replacement\n  bootstrap_indices <- sample(1:n, n, replace = TRUE)\n  x_b <- x[bootstrap_indices]\n  z_b <- z[bootstrap_indices]\n  r_b <- r[bootstrap_indices]\n  b_x_b <- b_x[bootstrap_indices]\n  df <- data.frame(r_b = r_b, b_x_b = b_x_b, z_b = z_b)\n  \n  # Use linear regression to estimate parameters for bootstrapped data\n  model_0_b <- lm(r_b ~ b_x_b, data = df[z_b == 0,])\n  model_1_b <- lm(r_b ~ b_x_b, data = df[z_b == 1,])\n  \n  hat_alpha_0_b <- coef(model_0_b)[1]\n  hat_alpha_1_b <- coef(model_1_b)[1]\n  hat_beta_0_b <- coef(model_0_b)[2]\n  hat_beta_1_b <- coef(model_1_b)[2]\n  \n  # Compute the estimator for bootstrapped data\n  b_bar_b <- mean(b_x_b)\n  ATE_boot[i] <- (hat_alpha_1_b - hat_alpha_0_b) + (hat_beta_1_b - hat_beta_0_b) * b_bar_b\n}\n\n# Standard error is the standard deviation of the bootstrapped estimates\nSE_ATE_boot <- sd(ATE_boot)\nmean_ATE_boot <- mean(ATE_boot)\n\nprint(SE_ATE_boot) # print standard error for bootstrap estimated ATE\n\n[1] 0.1577429\n\nprint(mean_ATE_boot) # check bootstrap estimated ATE resembles above estimate\n\n[1] 1.433139\n\n\n\n# 9. Visualizations to Aid Intuition\nggplot(df, aes(x=b_x, fill=as.factor(z), color = as.factor(z))) +\n  geom_density(alpha = 0.15) +\n  geom_dotplot(\n    alpha = 0.5,\n    stackgroups = TRUE,\n    method = \"dotdensity\",\n    binpositions = 'all',\n    binwidth = .022\n  ) + \n  labs(title=\"Distribution of Propensity Scores by Treatment Group\",\n       x=\"Propensity Score\",\n       y=\"Frequency\",\n       fill=\"Treatment Group\",\n       color=\"Treatment Group\") + \n  scale_fill_manual(\n    values = c('0' = 'dodgerblue', '1' = 'orange'),\n    labels = c(expression(r[0]), expression(r[1]))\n  ) + \n  scale_color_manual(\n    values = c('0' = 'dodgerblue', '1' = 'orange'),\n    labels = c(expression(r[0]), expression(r[1]))\n  ) + \n  theme_bw() + \n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nggplot(df, aes(x = b_x, y = r, color = factor(z))) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = expression(paste(\n      \"Outcomes \", r[0], \" and \", r[1], \" vs. Propensity Score\"\n    )),\n    x = \"Propensity Score\",\n    y = expression(paste(\"Outcome: \", r[0], \" or \", r[1])),\n    color = \"Outcome Type\"\n  ) +\n  annotate(\n    geom = 'segment',\n    xend = b_bar,\n    yend = hat_alpha_1 + hat_beta_1 * b_bar,\n    x = b_bar,\n    y = hat_alpha_0 + hat_beta_0 * b_bar,\n    arrow = arrow(ends = 'both', length = unit(2, 'mm'))\n  ) +\n  annotate(\n    geom = 'label',\n    x = b_bar + .16,\n    y = mean(c(hat_alpha_0, hat_alpha_1)) + mean(c(hat_beta_0, hat_beta_1)) * b_bar + .15,\n    label = 'average treatment effect',\n    alpha = 0.8\n  ) +\n  scale_color_manual(\n    values = c('0' = 'dodgerblue', '1' = 'orange'),\n    labels = c(expression(r[0]), expression(r[1]))\n  ) +\n  theme_bw()"
  },
  {
    "objectID": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#claims-about-matching",
    "href": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#claims-about-matching",
    "title": "2  The Central Role of Propensity Scores",
    "section": "Claims about matching",
    "text": "Claims about matching\nRosenbaum and Rubin continue in their section on applications of propensity scores to observational studies to make the following arguments about the superiority of matching techniques to model-based adjustment ones in estimating average treatment effects.\nThey claim\n\nMatched treated and control pairs allow relatively unsophisticated researchers to do meaningful analysis by performing pair-comparisons.\nMatching methods should have lower variance estimates of treatment effects compared to covariance based methods because matched samples should be more similar than in random samples.\nModel-based adjustment on matched samples should be more robust to the assumed form of the underlying model than model-based adjustement on random samples because of reduced reliance on model extrapolations.\nIn settings with small numbers of treated observations, large reservoirs of control observations, and large numbers of potential confounders, those confounding variables are more likely to be able to be adjusted for by matched sampling than regression adjustment."
  },
  {
    "objectID": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#section-1",
    "href": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#section-1",
    "title": "2  The Central Role of Propensity Scores",
    "section": "",
    "text": "Definition. The initial bias in \\(X\\) is defined as the quantity \\[B = \\mathbb E[X|Z=1] - \\mathbb E[X|Z=0]\\]\nand the expected bias in \\(X\\) in matched samples is \\[ B_m = \\mathbb E[X|Z=1] - \\mathbb E_m [X|Z=0]\\] where the subscript \\(m\\) indicates the distribution in matched samples.\nIf \\(B_m = \\gamma B\\) for some scalar \\(\\gamma\\) with \\(0 < \\gamma < 1\\) then matching reduces bias in each coordinate of \\(X\\) is reduced by \\(100(1-\\gamma)\\)% and we say that the matching method is equal percent bias reducing."
  },
  {
    "objectID": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#theorem-6",
    "href": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#theorem-6",
    "title": "2  The Central Role of Propensity Scores",
    "section": "Theorem 6",
    "text": "Theorem 6\nLet \\(b = b(x)\\) be a balancing score. For any matching method that uses \\(b\\) alone to match each treated unit with a control unit, the reduction in bias is \\[B - B_m = \\int \\mathbb E[X|b] \\{ \\text{Pr}_m(b|z=0) - \\text{Pr}(b|z = 0) \\} db,\\] where \\(\\text{Pr}_m(b | z = 0)\\) denotes the distribution of \\(b\\) in matched samples from the control group.\nProof. Using the definitions introduced immediately prior, we have that \\[\\begin{aligned} B-B_m = \\int \\{ E_m&[X | Z = 0, b) \\text{Pr}_m(b|Z=0) \\\\\n& - \\mathbb E[X|Z=0,b) \\text{Pr}(b|Z=0) \\} db.\\end{aligned}\\] For any matching method satisfying the condition of the theorem \\[\\mathbb E_m[X|Z=0,b] = \\mathbb E[X|Z=0,b]\\] because any matching method using \\(b\\) alone to match units alters the marginal distribution of \\(b\\) in the control group, \\(Z = 0\\), but does not alter the conditional distribution of \\(x\\) given \\(b\\) in the control group. However, by theorem 2, \\[\\mathbb E[X|Z=0,b] = \\mathbb E[X|b].\\] Substitution into the first equation yields the result.\n\n\\(\\square\\)"
  },
  {
    "objectID": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#corollary-6.1",
    "href": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#corollary-6.1",
    "title": "2  The Central Role of Propensity Scores",
    "section": "Corollary 6.1",
    "text": "Corollary 6.1\nIf \\(\\mathbb E[X|b] = \\alpha + \\beta f(b)\\) for some vectors \\(\\alpha\\) and \\(\\beta\\) and some scalar-valued function \\(f(\\cdot)\\), then matching on \\(b\\) alone is equal percent bias reducing.\nProof. The percent reduction in bias for the \\(i\\)th coordinate of \\(X\\) is:\n\\[ 100 \\frac{\\beta_i [\\mathbb E_m(f(b)|Z=0) - \\mathbb E(f(b)|Z=0)]}{\\beta_i [\\mathbb E(f(b)|Z=1) - \\mathbb E(f(b)|Z=0)]},\\] which is independent of \\(i\\) as required (because \\(\\beta_i\\) cancels from the numerator and denominator)."
  },
  {
    "objectID": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#theorem-7",
    "href": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#theorem-7",
    "title": "2  The Central Role of Propensity Scores",
    "section": "Theorem 7",
    "text": "Theorem 7\nLet \\(I_s\\) be the set of values of a balancing score which make up subclass \\(s\\) (\\(s = 1, ..., S\\)), so that \\(b(a) \\in I_s\\) implies that the units with \\(x = a\\) fall in subclass \\(s\\). Suppose the weight applied to subclass \\(s\\) in direct adjustment is \\(w_s\\).\nThe bias in \\(x\\) after direct adjustment for the subclasses \\((I_s, s = 1, ..., S)\\) is \\[\\begin{aligned}B_i = \\sum_{s = 1}^S w_s \\int \\mathbb E(X|b) & \\{ \\text{Pr}(b|Z=1, b \\in I_s) \\\\\n& - \\text{Pr}(b|Z=0, b \\in I_s \\} db,\\end{aligned}\\] where \\(b = b(x)\\).\nFor pedagogical aid, let’s go back to our first example and consider what bias results from dividing the dataset into varying fineness of quantile bins (tertiles, quintiles, and deciles) and observe how bias decreases as we increase the fineness of our bins. In this simulation, we use 1000 observations to ensure we have large enough sample sizes in each bin.\n\n\n\n\n# Generate a data.frame of 1000 folks with ages from 19-65\ndf <- data.frame(x = sample(x=seq(19,65), replace=TRUE, size = 1000))\n\n# Coefficients for our logistic model\nbeta_0 <- -5\nbeta_1 <- 0.1\n\n# Compute the odds of treatment using a logistic function\ndf$e_x <- exp(beta_0 + beta_1 * df$x) / (1 + exp(beta_0 + beta_1 * df$x))\n\n# Assign treatment group based on the computed odds\ndf$z <- rbinom(n = nrow(df), size = 1, prob = df$e_x)\n\n# Calculate the initial bias in x\ninitial_bias_in_x <-\n  df %>% filter(z == 1) %>% summarize(x = mean(x)) -\n  df %>% filter(z == 0) %>% summarize(x = mean(x))\n\n# Describe initial bias\ncat(paste0(\"Initial bias in x (age): \", round(initial_bias_in_x, 1), \"\\n\"))\n\nInitial bias in x (age): 15.5\n\ncat(stringr::str_wrap(\n  paste0(\n    \"In other words, age is on average \",\n    round(initial_bias_in_x, 1),\n    \" years higher in the treated group than the untreated group\"\n  ),\n  width = 80\n))\n\nIn other words, age is on average 15.5 years higher in the treated group than\nthe untreated group\n\n# Consider tertile, quantile, and decile subclassification\nfor (n in c(3, 5, 10)) {\n  # Divide propensity scores into S subclasses.\n  df$quantile <- cut(df$e_x, quantile(df$e_x, seq(0,1,length.out=n)), include.lowest = TRUE)\n  \n  # expectation of x by quantile of propensity score and by treatment status\n  E_x_by_subclass <- df %>% \n    group_by(quantile, z) %>%\n    summarise(E_x_b = mean(x))\n  \n  # pivot wider so we can subtract\n  E_x_by_subclass <- tidyr::pivot_wider(E_x_by_subclass,\n    id_cols = 'quantile', \n    values_from = 'E_x_b',\n    names_from = z)\n  \n  # rename columns from 1 and 0 to be more descriptive\n  E_x_by_subclass <-\n    rename(\n      E_x_by_subclass,\n      E_x_given_subclass_and_z1 = `1`,\n      E_x_given_subclass_and_z0 = `0`\n    )\n  \n  # use equal weighting for each subclass\n  w_s <- 1/n\n  \n  # calculate the bias in x within the Is substrata \n  B_s <- sum(w_s * with(E_x_by_subclass, (E_x_given_subclass_and_z1 - E_x_given_subclass_and_z0)))\n  \n  cat(paste0(\"Bias in x (age) after adjusting for \", n, \" subclasses is \", round(B_s, 1), '\\n'))\n}\n\nBias in x (age) after adjusting for 3 subclasses is 3\nBias in x (age) after adjusting for 5 subclasses is 0.8\nBias in x (age) after adjusting for 10 subclasses is 0.1"
  },
  {
    "objectID": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#my-conclusions",
    "href": "1983_Rosenbaum_and_Rubin/1983_Rosenbaum_and_Rubin.html#my-conclusions",
    "title": "2  The Central Role of Propensity Scores",
    "section": "My Conclusions",
    "text": "My Conclusions\nThe first comment I would make is that for anyone looking to follow the details of the proofs, the presentation in Chapters 12 and 13 of Causal Inference for Statistics, Social, and Biomedical Sciences by Guido W. Imbens and Donald B. Rubin is much more introductory, detailed and I found the notation more clear in terms of what is a random variable vs. being treated as a fixed value.\nI did not realize to what an extent this paper would be a mini-treatise on the advantages of matching as opposed to covariance-adjustment methods when using propensity scores.\nI’m very happy to feel that I’m now up to speed on what the proofs behind the scenes are when people talk about propensity score matching or regression methods that incorporate propensity scores. I’ll have to keep reading as I’ve just started to discover some literature around the deficiencies of propensity score matching, which I now feel prepared to tackle given my newfound acquaintance with the subject matter. For example of criticisms, see:\n\nKing, G., & Nielsen, R. (2019). Why Propensity Scores Should Not Be Used for Matching. Political Analysis, 27(4), 435-454. doi:10.1017/pan.2019.11\n\nMy sense is that it’s been a fruitful task for me to work through this paper filling in the proof details that weren’t immediately clear to me and writing simulation code to aid learning as I went along. I’m hopeful that this knowledge will pay off as I move towards more complex subjects, especially things like  targeted learning or super-learners."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Readings in Causal Inference, Fall 2023",
    "section": "Introduction",
    "text": "Introduction\nDuring the Fall 2023 I’m doing a reading-based independent study and writing up my notes to share here from some classic and modern texts in causal inference with a particular interest on a few subtopics including 1) complex mediators, 2) the role of semiparametric methods, and 3) doubly-robust learning.\n\n\n\n\n\n\nReading List for the Semester\n\n\n\n\n\n\n\n\nArticle Reference\nLink to Article/Book\n\n\n\n\nRosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1), 41-55.\nArticle\n\n\nRobins, J. M., & Ritov, Y. (1997). Toward a curse of dimensionality appropriate (CODA) asymptotic theory for semi-parametric models. Statistics in medicine, 16(3), 285-319.\nArticle\n\n\nRobins, J. M., Hernán, M. Á., & Brumback, B. (2000). Marginal structural models and causal inference in epidemiology. Epidemiology, 11(5), 550-560.\nArticle\n\n\nDawid, A. P. (2000). Causal inference without counterfactuals. Journal of the American Statistical Association, 95(450), 407-424.\nArticle\n\n\nPearl, J. (2001). Direct and indirect effects. Proceedings of the seventeenth conference on uncertainty in artificial intelligence.\nArticle\n\n\nZhang, Z., & Rubin, D. B. (2003). Estimation of causal effects via principal stratification when some outcomes are truncated by ‘death’. Journal of Educational and Behavioral Statistics, 28(4), 353-368.\nArticle\n\n\nRubin, D. B. (2005). Causal inference using potential outcomes: Design, modeling, decisions. Journal of the American Statistical Association, 100(469), 322-331.\nArticle\n\n\nHernán, M. A., & Robins, J. M. (2006). Instruments for causal inference: an epidemiologist’s dream?. Epidemiology, 17(4), 360-372.\nArticle\n\n\nShalizi, C. R., & Thomas, A. C. (2010). Homophily and contagion are generically confounded in observational social network studies. Sociological methods & research, 40(2), 211-239.\nArticle\n\n\nVan der Laan, M. J., & Rose, S. (2011). Targeted Learning: Causal Inference for Observational and Experimental Data. Springer.\nBook\n\n\nPeng, R. D. (2011). Reproducible research in computational science. Science, 334(6060), 1226-1227.\nArticle\n\n\nTchetgen Tchetgen, E. J., & Shpitser, I. (2012). Semiparametric theory for causal mediation analysis: efficiency bounds, multiple robustness and sensitivity analysis. The Annals of Statistics, 40(3), 1816-1845.\nArticle\n\n\nPearl, J. (2012). The causal mediation formula—a guide to the assessment of pathways and mechanisms. Prevention science, 13(4), 426-436.\nArticle\n\n\nTchetgen Tchetgen, E. J. (2013). Identification and estimation of survivor average causal effects. Biometrika, 100(2), 503-518.\nArticle\n\n\nTchetgen Tchetgen, E. J. (2014). The control outcome calibration approach for causal inference with unobserved confounding. The American Statistician, 68(1), 27-32.\nArticle\n\n\nVanderWeele, T. J., & Tchetgen Tchetgen, E. J. (2014). Mediation analysis with time varying exposures and mediators. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(3), 523-545.\nArticle\n\n\nVanderWeele, T. J. (2015). Explanation in causal inference: methods for mediation and interaction. Oxford University Press.\nBook\n\n\nImbens, G. W., & Rubin, D. B. (2015). Causal Inference in Statistics, Social, and Biomedical Sciences. Cambridge University Press.\nBook\n\n\nPeters, J., Janzing, D., & Schölkopf, B. (2017). Elements of causal inference: foundations and learning algorithms. MIT press.\nBook\n\n\nRobins, J. M., Sued, M., Lei, G., & Mínguez, D. (2017). Comment: The self-controlled case series method - An innovative design for home and online randomized controlled trials. Statistical Science, 32(2), 264-267.\nArticle\n\n\nHernán, M. A., & Robins, J. M. (2018). Causal Inference. Chapman & Hall/CRC.\nBook\n\n\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21(1), C1-C68.\nLink\n\n\n\n\n\n\nSupplemental Articles\nLink to Article\n\n\n\n\nLu Cheng, Ruocheng Guo, and Huan Liu. 2022. Causal Mediation Analysis with Hidden Confounders. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM ’22). Association for Computing Machinery, New York, NY, USA.\nArticle\n\n\nZhang, Z., Zheng, W., & Albert, J. M. (2019). High-dimensional mediation analysis with latent variables in randomized clinical trials. Statistical Methods in Medical Research, 28(10-11), 3186-3200.\nArticle\n\n\nDerkach A, Pfeiffer RM, Chen TH, Sampson JN. High dimensional mediation analysis with latent variables. Biometrics. 2019 May 5.\nArticle\n\n\nZeng P, Shao Z, Zhou X. Statistical methods for mediation analysis in the era of high-throughput genomics: Current successes and future challenges. Computational and Structural Biotechnology Journal. 2021.\nArticle\n\n\nEric J. Tchetgen Tchetgen. Ilya Shpitser. “Semiparametric theory for causal mediation analysis: Efficiency bounds, multiple robustness and sensitivity analysis.” Ann. Statist. June 2012. https://doi.org/10.1214/12-AOS990\nArticle\n\n\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations. arXiv preprint arXiv:1802.05365.\nArticle\n\n\nNima S Hejazi and others, Nonparametric causal mediation analysis for stochastic interventional (in)direct effects, Biostatistics, July 2023. Article\nArticle\n\n\nHernán, M., Taubman, S. Does obesity shorten life? The importance of well-defined interventions to answer causal questions. Int J Obes 32 (Suppl 3), S8–S14 (2008). https://doi.org/10.1038/ijo.2008.82\nArticle\n\n\nPeters, J., Mooij, J. M., Janzing, D., & Schölkopf, B. (2014). Causal discovery with continuous additive noise models.\nArticle\n\n\nMooij, J. M., Peters, J., Janzing, D., Zscheischler, J., & Schölkopf, B. (2016). Distinguishing cause from effect using observational data: methods and benchmarks. The Journal of Machine Learning Research.\nArticle"
  },
  {
    "objectID": "index.html#writeups",
    "href": "index.html#writeups",
    "title": "Readings in Causal Inference, Fall 2023",
    "section": "Writeups",
    "text": "Writeups\n\nWeek 1: The Central Role of the Propensity Score in Observational Studies for Causal Effects by Paul R. Rosenbaum and Donald B. Rubin (1983), Biometrika\nHTML  PDF\n\nWeek 2a: Marginal Structural Models for Causal Inference by James M. Robin, Miguel Ángel Hernán, and Babette Brumback (2000), Epidemiology\nHTML  PDF\n\nWeek 2b: Causal Inference Without Counterfactuals  by Alexander Phillip Dawid (2000). JASA\nHTML  PDF\n\nWeek 3: Foundations of Agnostic Statistics  by Peter M. Aronow and Benjamin T. Miller (2019). Cambridge University Press.\nHTML"
  },
  {
    "objectID": "2000_Dawid/2000_Dawid.html",
    "href": "2000_Dawid/2000_Dawid.html",
    "title": "4  Causal Inference Without Counterfactuals",
    "section": "",
    "text": "5 Responses"
  },
  {
    "objectID": "2000_Dawid/2000_Dawid.html#types-of-causal-inference",
    "href": "2000_Dawid/2000_Dawid.html#types-of-causal-inference",
    "title": "4  Causal Inference Without Counterfactuals",
    "section": "Types of Causal Inference",
    "text": "Types of Causal Inference\nDawid draws our attention to the difference in nature between two types of questions, which can be characterized by archetypal examples.\n\n“I have a headache. Will it help if I take aspirin?”\n“My headache has gone. Was it because I took aspirin?”\n\nWhile Dawid contrasts these as being about “effects of causes” and “causes of effects,” I find that language is a little bit tricky to follow. Perhaps it is because of my prior knowledge, but I immediately recast these in my mind into the language of “token” and “type” causation, where token causation refers to causal inference with respect to a specific event and type causation refers to causal inference regarding trends in events. In other words, we could take climate related events as an example: “Did climate change cause this storm?” and “Does climate change increase storm intensity?” The former is an example of token causation and in general is thought by many to be much harder to establish. The latter is an example of a question about type causation and is more firmly in the realm of what we usually think of when we think about observational causal inference questions.\nIn the case of Dawid’s examples, I view question 1 as being a question of type causation because at-best we can apply our knowledge about trends in outcomes under aspirin treatment and non-treatment to make some prediction about what might happen in the future under aspirin treatment and non-treatment circumstances. Meanwhile, I view question 2 as a question of token causation: without knowing much more, it’s hard to know if the person in question was simply dehydrated and the water they drank with their aspirin was the more causally significant explanatory variable or perhaps the headache just went away on its own.\nLater on, Dawid has this to say about what I would call inference regarding token causation:\n\nNo amount of wishful thinking, clever analysis, or arbitrary untestable assumptions can license unambiguous inference about causes of effects, even when the model is simple and the data are extensive (unless one is lucky enough to discover uniformity among units).\n\n“Wishful thinking” and “clever analysis,” I agree with, but arbitrary untestable assumptions can get you pretty far — But I agree with him in principle here. I think it’s practically impossible to take a recorded history and give a rigorous treatment to why it played out the way it did using statistical machinery, as far as I can tell. This is in large part due to the problem of fundamental causes: eventually we’ll get to a statement like “X happened because the Universe began” which is profoundly unsatisfying."
  },
  {
    "objectID": "2000_Dawid/2000_Dawid.html#a-decision-analytic-framework",
    "href": "2000_Dawid/2000_Dawid.html#a-decision-analytic-framework",
    "title": "4  Causal Inference Without Counterfactuals",
    "section": "A Decision-Analytic Framework",
    "text": "A Decision-Analytic Framework\nI find this a bit strange, but Dawid claims that “the counterfactual approach typically takes as the fundamental object of causal inference the  individual causal effect…”\nI can’t tell if it’s that I’m not very familiar with what the field of causal inference looked like pre-1980 or so, or if it’s just that this is a mischaracterization. Either way, what I’m familiar with is exclusively causal inference literature that takes as its main object of interest the average treatment effect or another population-level treatment effect (perhaps conditioned if thinking about effect-heterogeneity).\nNonetheless, Dawid mounts criticism of the counterfactual framework as involving “metaphysical” variables (variables we do not directly observe) and suggests a Bayesian decision-analytic alternative to the counterfactual formulation of causal inference instead.\nRather than investigating the properties of \\(Y_1 - Y_0\\) (the individual causal effect) (or even \\(\\mathbb E[Y_1] - \\mathbb E[Y_0]\\), the average treatment effect), instead Dawid suggests that what we should do is:\n\nRestrict our studies to perfectly homogeneous populations so as to banish any potential confounders;\nLook at the “physical array” of values representing the outcomes of treated and untreated units to develop a prediction model;\nAnd for any questions about whether or not treatment should be assigned in the future, we should use a Bayesian decision-theoretic model to infer the optimality choice based on the distributions of observed outcomes for treated vs. untreated.\n\nDawid does grant that models could be developed for nonhomogeneous populations, suggesting that “symmetry arguments” can be used to “justify the construction of certain random-effects-type models …”\nHe goes on to say, “As long as one’s models relate the responses of the new and the old units (under arbitrary treatment assignments), and so support the required predictive inferences, one can conduct whatever decision-analytic analysis appears most relevant to one’s purpose, eschewing counterfactuals entirely.”\nIt seems to me that one of the following two statements must be true, depending on how comfortable Dawid is with the line of thought that the decision-analytic framework could support nonhomogeneous populations:\n\nRestricting causal inference to only homogeneous populations would be incredibly restrictive and would render a huge number of problems for which data already exist in economics, sociology, health sciences, and education un-workable.\nIf we’re allowing ourselves to control for potential confounders in prediction models that are used for a Bayesian decision-analytic approach, I’m suspicious we’re treading awfully close to a modeling approach that, while in its description may sound different, is fundamentally doing something extremely similar to the usual counterfactual approach."
  },
  {
    "objectID": "2000_Dawid/2000_Dawid.html#philosophy-and-fatalism",
    "href": "2000_Dawid/2000_Dawid.html#philosophy-and-fatalism",
    "title": "4  Causal Inference Without Counterfactuals",
    "section": "Philosophy and “Fatalism”",
    "text": "Philosophy and “Fatalism”\n\nMany counterfactual analyses are based, explicitly or implicity, on an attitude that I term “fatalism.” This considers the various potential responses \\(Y_i(u)\\), when treatment \\(i\\) is applied to unit \\(u\\), as predetermined attributes of unit \\(u\\), waiting only to be uncovered by suitable experimentation.\n\nIt’s a little bit hard here to disentangle whether Dawid is talking about 1) whether the Universe is a deterministic or stochastic place, or 2) whether he’s saying that counterfactuals are themselves random or fixed variables.\nHowever, he goes on to say:\n\nFor example, it [the fatalistic worldview] leaves no scope for introducing realistic stochastic effects of external influences acting between times of application of treatment and of the response.\n\nSo it sounds a lot like he’s saying he thinks that much of the approach of counterfactual causal inference is rooted in an assumption that the counterfactual is itself not a random variable, which I just simply don’t agree with as that’s not a presentation or approach I’ve ever seen advocated for."
  },
  {
    "objectID": "2000_Dawid/2000_Dawid.html#have-we-constructed-a-strawman",
    "href": "2000_Dawid/2000_Dawid.html#have-we-constructed-a-strawman",
    "title": "4  Causal Inference Without Counterfactuals",
    "section": "Have we constructed a strawman?",
    "text": "Have we constructed a strawman?\nAt one point (section 9.1), Dawid says that the average causal effect (in his notation: \\(\\mathbb E_p\\{f(Y_t) - f(Y_c)\\}\\)) (I take in a population where exchangeability, positivity, and consistency hold) is just \\(\\mathbb E_P \\{f(Y_t)\\} - \\mathbb E_P \\{ f(Y_c) \\}\\) and this only depends on the marginal distributions.\n\nHence this particular use of counterfactual analysis, focusing on an infinite population ACE, is consistent with the decision-analytic approach and involves only terms subject to empirical scrutiny. It is fortunate that many of the superficially counterfactual analyses in the literature, from Rubin (1978) onward, have in fact confined attention to ACE and thus lead to acceptable conclusions.\n\nAt this point, I’m questioning what exactly Dawid is attacking given that he clearly says much of the causal inference literature is acceptable.\n\n“Conclusions”\n\nThere is no magical statistical route that can bypass the need to do real science to attain the clearest possible understanding of the operation of relevant (typically nondeterministic) causal mechanisms.\n\nThere’s a lot to unpack there. First, I think it’s a real bold move to assume that you can litigate what “real” science is and isn’t. Second, it’s news to me if we’ve settled the debate on whether the universe is deterministic or not, but insofar as its relevant to this discussion, it doesn’t really matter as long as long as the best models we have for sufficiently complex future outcomes are those of random data generation mechanisms."
  },
  {
    "objectID": "2000_Dawid/2000_Dawid.html#conclusions",
    "href": "2000_Dawid/2000_Dawid.html#conclusions",
    "title": "4  Causal Inference Without Counterfactuals",
    "section": "“Conclusions”",
    "text": "“Conclusions”\n\nThere is no magical statistical route that can bypass the need to do real science to attain the clearest possible understanding of the operation of relevant (typically nondeterministic) causal mechanisms.\n\nThere’s a lot to unpack there. First, I think it’s a real bold move to assume that you can litigate what “real” science is and isn’t. Second, it’s news to me if we’ve settled the debate on whether the universe is deterministic or not, but insofar as its relevant to this discussion, it doesn’t really matter as long as long as the best models we have for sufficiently complex future outcomes are those of random data generation mechanisms."
  },
  {
    "objectID": "2000_Dawid/2000_Dawid.html#cox",
    "href": "2000_Dawid/2000_Dawid.html#cox",
    "title": "4  Causal Inference Without Counterfactuals",
    "section": "Cox",
    "text": "Cox\nOf all the responses, I thought the one that hit the nail on the head was David R. Cox’s:\n\nhas the philosophical coherence, if not thrown the baby out with the bathwater, at least left the baby seriously bruised in some vital organs?\n\nIn other words: is Dawid making perfect the enemy of good?\nI agree with Cox that “it is hard to disagree with Dawid’s distate for assumptions that can never be tested… Yet at a work-a-day level, the point is more that any assumptions should not be pressed too far beyond the limits to which they can be tested, and importantly, that assumptions can be tested indirectly via their consequences as well as directly.”\nI think Cox also raises an interesting philosophical question about causality, which is that is causal inference truly causal if it lacks explanation? As in, if a carefully randomized experiment (or even series of experiments) shows that T produces higher responses than C, but no understanding as to how is established, is it the case that causality has been established? “In one sense it has, and yet I believe that many working scientists would be uneasy using the term in such situations.”"
  },
  {
    "objectID": "2000_Dawid/2000_Dawid.html#casella-and-schwartz",
    "href": "2000_Dawid/2000_Dawid.html#casella-and-schwartz",
    "title": "4  Causal Inference Without Counterfactuals",
    "section": "Casella and Schwartz",
    "text": "Casella and Schwartz\n\nDawid insists that such choices, and inferences, must be based on strict principles that can be verified empirically. We believe that such a program is so overly rigid that, in the end, science is not served.\n\nOf particular note to me is the historical evidence that Casella and Schwartz bring to the table — already in 1748, philosopher David Hume was thinking about causal inference and making the distinction of token vs. type causation:\n\nIt appears, then, that this idea of a necessary connection among events arises from a number of similar instances which occur, of the constant conjunction of these events; nor can that idea ever be suggested by any one of these instances surveyed in all possible lights and positions.\n\nCasella and Schwartz also note that it feels like Dawid has substantially changed the inferential target of interest from the average causal effect to the decision-theoretically important quantity \\(u_0 | \\text{treatment} = t\\)."
  },
  {
    "objectID": "2000_Dawid/2000_Dawid.html#pearl",
    "href": "2000_Dawid/2000_Dawid.html#pearl",
    "title": "4  Causal Inference Without Counterfactuals",
    "section": "Pearl",
    "text": "Pearl"
  },
  {
    "objectID": "2000_Dawid/2000_Dawid.html#dawids-article",
    "href": "2000_Dawid/2000_Dawid.html#dawids-article",
    "title": "4  Causal Inference Without Counterfactuals",
    "section": "Dawid’s Article",
    "text": "Dawid’s Article\nDawid argues that because we never observe both treated and untreated outcomes for the same units, we are dangerously wading into metaphysical terrain and that ultimately this renders causal inference in its counterfactual formulation untestable and unfalsifiable. I suppose I’m giving away the ending, but a large handful of eminent statisticians wrote responses to this article disagreeing quite substantially with its premises.\nDawid does have a few choice quotes that I particularly agree with:\n\nNature is surely utterly indifferent to our attempts to ensnare her in our theories.\n\n\nAs long as a model appears to describe the relevant aspects of the world satisfactorily, we may continue, cautiously to use it; when it fails to do so, we need to search for a better one.\n\nAnd here we get into the substance of issue:\n\nMy approach is grounded in a Popperian philosophy, in which the meaningfulness of a purportedly scientific theory, proposition, quantity, or concept is related to the implications it has for what is or could be observed … When this is the case, assertions are empirically refutable and are considered “scientific.” When this is not so, they may be branded “metaphysical.” I argue that counterfactual theories are essentially metaphysical.\n\nWe’ll be discussing what this “Popperian philosophy” exactly is and the role philosophy of science has in this conversation at length, but for now let’s establish a few more facts about Dawid’s argument.\n\nTypes of Causal Inference\nDawid draws our attention to the difference in nature between two types of questions, which can be characterized by archetypal examples.\n\n“I have a headache. Will it help if I take aspirin?”\n“My headache has gone. Was it because I took aspirin?”\n\nWhile Dawid contrasts these as being about “effects of causes” and “causes of effects,” I find that language is a little bit tricky to follow. Perhaps it is because of my prior knowledge, but I immediately recast these in my mind into the language of “token” and “type” causation, where token causation refers to causal inference with respect to a specific event and type causation refers to causal inference regarding trends in events. In other words, we could take climate related events as an example: “Did climate change cause this storm?” and “Does climate change increase storm intensity?” The former is an example of token causation and in general is thought by many to be much harder to establish. The latter is an example of a question about type causation and is more firmly in the realm of what we usually think of when we think about observational causal inference questions.\nIn the case of Dawid’s examples, I view question 1 as being a question of type causation because at-best we can apply our knowledge about trends in outcomes under aspirin treatment and non-treatment to make some prediction about what might happen in the future under aspirin treatment and non-treatment circumstances. Meanwhile, I view question 2 as a question of token causation: without knowing much more, it’s hard to know if the person in question was simply dehydrated and the water they drank with their aspirin was the more causally significant explanatory variable or perhaps the headache just went away on its own.\nLater on, Dawid has this to say about what I would call inference regarding token causation:\n\nNo amount of wishful thinking, clever analysis, or arbitrary untestable assumptions can license unambiguous inference about causes of effects, even when the model is simple and the data are extensive (unless one is lucky enough to discover uniformity among units).\n\n“Wishful thinking” and “clever analysis,” I agree with, but arbitrary untestable assumptions can get you pretty far — But I agree with him in principle here. I think it’s practically impossible to take a recorded history and give a rigorous treatment to why it played out the way it did using statistical machinery, as far as I can tell. This is in large part due to the problem of fundamental causes: eventually we’ll get to a statement like “X happened because the Universe began” which is profoundly unsatisfying.\n\n\nA Decision-Analytic Framework\nI find this a bit strange, but Dawid claims that “the counterfactual approach typically takes as the fundamental object of causal inference the  individual causal effect…”\nI can’t tell if it’s that I’m not very familiar with what the field of causal inference looked like pre-1980 or so, or if it’s just that this is a mischaracterization. Either way, what I’m familiar with is exclusively causal inference literature that takes as its main object of interest the average treatment effect or another population-level treatment effect (perhaps conditioned if thinking about effect-heterogeneity).\nNonetheless, Dawid mounts criticism of the counterfactual framework as involving “metaphysical” variables (variables we do not directly observe) and suggests a Bayesian decision-analytic alternative to the counterfactual formulation of causal inference instead.\nRather than investigating the properties of \\(Y_1 - Y_0\\) (the individual causal effect) (or even \\(\\mathbb E[Y_1] - \\mathbb E[Y_0]\\), the average treatment effect), instead Dawid suggests that what we should do is:\n\nRestrict our studies to perfectly homogeneous populations so as to banish any potential confounders;\nLook at the “physical array” of values representing the outcomes of treated and untreated units to develop a prediction model;\nAnd for any questions about whether or not treatment should be assigned in the future, we should use a Bayesian decision-theoretic model to infer the optimality choice based on the distributions of observed outcomes for treated vs. untreated.\n\nDawid does grant that models could be developed for nonhomogeneous populations, suggesting that “symmetry arguments” can be used to “justify the construction of certain random-effects-type models …”\nHe goes on to say, “As long as one’s models relate the responses of the new and the old units (under arbitrary treatment assignments), and so support the required predictive inferences, one can conduct whatever decision-analytic analysis appears most relevant to one’s purpose, eschewing counterfactuals entirely.”\nIt seems to me that one of the following two statements must be true, depending on how comfortable Dawid is with the line of thought that the decision-analytic framework could support nonhomogeneous populations:\n\nRestricting causal inference to only homogeneous populations would be incredibly restrictive and would render a huge number of problems for which data already exist in economics, sociology, health sciences, and education un-workable.\nIf we’re allowing ourselves to control for potential confounders in prediction models that are used for a Bayesian decision-analytic approach, I’m suspicious we’re treading awfully close to a modeling approach that, while in its description may sound different, is fundamentally doing something extremely similar to the usual counterfactual approach.\n\n\n\nPhilosophy and “Fatalism”\n\nMany counterfactual analyses are based, explicitly or implicity, on an attitude that I term “fatalism.” This considers the various potential responses \\(Y_i(u)\\), when treatment \\(i\\) is applied to unit \\(u\\), as predetermined attributes of unit \\(u\\), waiting only to be uncovered by suitable experimentation.\n\nIt’s a little bit hard here to disentangle whether Dawid is talking about 1) whether the Universe is a deterministic or stochastic place, or 2) whether he’s saying that counterfactuals are themselves random or fixed variables.\nHowever, he goes on to say:\n\nFor example, it [the fatalistic worldview] leaves no scope for introducing realistic stochastic effects of external influences acting between times of application of treatment and of the response.\n\nSo it sounds a lot like he’s saying he thinks that much of the approach of counterfactual causal inference is rooted in an assumption that the counterfactual is itself not a random variable, which I just simply don’t agree with as that’s not a presentation or approach I’ve ever seen advocated for.\n\n\nHave we constructed a strawman?\nAt one point (section 9.1), Dawid says that the average causal effect (in his notation: \\(\\mathbb E_p\\{f(Y_t) - f(Y_c)\\}\\)) (I take in a population where exchangeability, positivity, and consistency hold) is just \\(\\mathbb E_P \\{f(Y_t)\\} - \\mathbb E_P \\{ f(Y_c) \\}\\) and this only depends on the marginal distributions.\n\nHence this particular use of counterfactual analysis, focusing on an infinite population ACE, is consistent with the decision-analytic approach and involves only terms subject to empirical scrutiny. It is fortunate that many of the superficially counterfactual analyses in the literature, from Rubin (1978) onward, have in fact confined attention to ACE and thus lead to acceptable conclusions.\n\nAt this point, I’m questioning what exactly Dawid is attacking given that he clearly says much of the causal inference literature is acceptable.\n\n\n“Conclusions”\n\nThere is no magical statistical route that can bypass the need to do real science to attain the clearest possible understanding of the operation of relevant (typically nondeterministic) causal mechanisms.\n\nThere’s a lot to unpack there. First, I think it’s a real bold move to assume that you can litigate what “real” science is and isn’t. Second, it’s news to me if we’ve settled the debate on whether the universe is deterministic or not, but insofar as its relevant to this discussion, it doesn’t really matter as long as long as the best models we have for sufficiently complex future outcomes are those of random data generation mechanisms."
  },
  {
    "objectID": "2000_Dawid/2000_Dawid.html#responses",
    "href": "2000_Dawid/2000_Dawid.html#responses",
    "title": "4  Causal Inference Without Counterfactuals",
    "section": "Responses",
    "text": "Responses\n\nCox\nOf all the responses, I thought the one that hit the nail on the head was David R. Cox’s:\n\nhas the philosophical coherence, if not thrown the baby out with the bathwater, at least left the baby seriously bruised in some vital organs?\n\nIn other words: is Dawid making perfect the enemy of good?\nI agree with Cox that “it is hard to disagree with Dawid’s distate for assumptions that can never be tested… Yet at a work-a-day level, the point is more that any assumptions should not be pressed too far beyond the limits to which they can be tested, and importantly, that assumptions can be tested indirectly via their consequences as well as directly.”\nI think Cox also raises an interesting philosophical question about causality, which is that is causal inference truly causal if it lacks explanation? As in, if a carefully randomized experiment (or even series of experiments) shows that T produces higher responses than C, but no understanding as to how is established, is it the case that causality has been established? “In one sense it has, and yet I believe that many working scientists would be uneasy using the term in such situations.”\n\n\nCasella and Schwartz\n\nDawid insists that such choices, and inferences, must be based on strict principles that can be verified empirically. We believe that such a program is so overly rigid that, in the end, science is not served.\n\nOf particular note to me is the historical evidence that Casella and Schwartz bring to the table — already in 1748, philosopher David Hume was thinking about causal inference and making the distinction of token vs. type causation:\n\nIt appears, then, that this idea of a necessary connection among events arises from a number of similar instances which occur, of the constant conjunction of these events; nor can that idea ever be suggested by any one of these instances surveyed in all possible lights and positions.\n\nCasella and Schwartz also note that it feels like Dawid has substantially changed the inferential target of interest from the average causal effect to the decision-theoretically important quantity \\(u_0 | \\text{treatment} = t\\).\n\n\nPearl\nPearl calls our attention to the historical track record of criticism on account of “metaphysics”:\n\nThe field of statistics has seen many well-meaning crusades against threats from metaphysics and other heresy. In its founding prospectus of 1834, the Royal Statistical Society resolved “to exclude carefully all Opinions from its transactions and publication——to confine its attention rigorously to facts.” This clause was officially struck out in 1858, when it became obvious that facts void of theory could not take statistics very far (Annals of the Royal Statistical Society 1934, p. 16).\n\nHe goes on to argue that the word “counterfactual” itself is a bit of a misnomer, and that any scientific law that establishes relationships between observable variables remains invariant when the values of those variables change. As an example, he provides a counterfactual-esque reading of Ohm’s law (V = IR) in which we could say if the resistance and voltage in a circuit were particular values, then we can solve for the current in that circuit. Pearl doesn’t quite come out and say it, but the fundamental “metaphysical” assumption here is that the laws of physics are invariant across time and space. Second, functions in general can be thought of as counterfactuals: if we substitute a particular value of \\(x\\), then we can use the rules of algebra and arithmetic to calculate the value of \\(f(x)\\). I thought this was one of the most damning criticisms of Dawid’s argument.\nPearl launches what is, in my view, another devastating (though less well supported) attack against Dawid’s line of thought— if we limit the language of science to exclusively entertain purely falsifiable ideas, what happens to inspiration and creativity in scientific pursuits? As examples, Pearl brings up how it was the Greek astronmers with their mythical creative-speculative strategies wild with metaphysical imagery of circular tubes, full of fire, and a hemispherical Earth riding on turtle backs that inspired Eratosthenes (276-194 BC) to design an experiment to measure the radius of the Earth (and it wasn’t the exacting, black-box computational culture of the Babylonians). Second, he says that it was precisely due to early 20th century physicists daring to ask “metaphysical” questions about physical properties of electrons when electrons are un-observable that inspired quantum physics.\nIt is at once reasonable to me that demanding rigidity in our language and in our experimental designs could be stifling to science, and yet I am usually averse to “proof by example” as has been presented by Pearl. It is at once hard to interpret the examples Pearl has brought forward as anything but cherry-picked, and yet I believe that whole fields of dignified science (anthropology, sociology) can admit excellent work by finding what are archetypal examples of social processes and studying them.\nPearl’s conclusion:\n\n“The success of the counterfactual language stems from two ingredients necessary for scientific progress in general: (a) the use of modeling languages that are somewhat richer than the ones needed for routine predictions, and (b) the use of powerful mathematics to filter, rather than muzzle, the untestable queries that such languages tempt us to ask.\n\n\n\nJames Robins and Sander Greenland\n\nWe are confident that Dawid does not wish to join R. A. Fisher (1959) in thereby concluding that causal inferences from observational data are illegitimate, including the inference that cigarette smoking is a cause of lung cancer (Stolley 1991). If we are correct, then Dawid has no choice but to recognize the need for untestable assumptions.\n\n\nOur conclusion is not to reject counterfactual models, however, but rather to criticize models and measures of effect that depend on nonidentifiable features (Greenland 1987) and to develop semiparametric counterfactual models (i.e., structural nested models, marginal structural models, and models based on the g-computation algorithm) that place no restrictions on those features (Robins 1997, 1999). Our approach completely obviates Dawid’s concern.\n\n\n… to misquote the Bard, “the vagueness is not in our counterfactuals but in our attempt to make causal inferences from observational data.”\n\nIt took me a bit to figure out that the Bard they’re referring to might be Shakespeare, and they’re misquoting (deliberately) the line from the play Julius Caeser, “The fault, dear Brutus, is not in our stars, but in ourselves, that we are underlings.”\n“Popper made clear that falsifiability means a theory must have some observable predictions that would lead to its rejection were those predictions to fail, not that every feature of the theory be testable (Popper 1974).”\n\n\nRubin\n“I prefer the more generous expression ‘potential outcomes’ to ‘counterfactuals’ to describe the perspective, because as Dawid himself points out before his equation (5), it is only after treatment assignments are known that some potential outcomes become known, whereas others become counterfactual.\nRubin goes on to argue that Dawid has been overly dismissive of the value of the counterfactual perspective, naming both the effectiveness to which he has been able to teach it to students to their great insight, and the utility it holds in rendering otherwise complex seemingly-paradoxical questions like Lord’s paradox much more straightforward.\n\n\nShafer\nShafer is the only responder who seems to support Dawid’s view, saying “my main reservation about the article is that it does not take advantage of Dawid’s own path-breaking work on predictive probability … In his effort to find common ground with those who tout counterfactual variables, Dawid emphasizes the case of a finite homogeneous population, where optimal predictions are merely population average … In the end, Dawid concedes too much, especially on the topic of causes of effects.”\n\n\nWasserman\nWasserman continues the line of thought started by Rubin, arguing that Simpson’s paradox is so easily resolved when cast in counterfactual language. He disputes how the analogy to incompatible quantum variables may be misrepresentative, saying that our inability to measure two incompatible variables is built into the quantum mechanics, whereas this is not the case for all situations in which we might apply counterfactual causal inference. For example, in principle, an idealized experiment in which both \\(Y_0\\) and \\(Y_1\\) are observed is feasible if we allow ourselves to consider different treatment over time when either carryover effects are minimal or we can wait them out before the next trial.\n\nI suggest that we continue to use counterfactuals but educate users to resist the temptation to indiscriminantly make inferences for nonidentified parameters in all models, not just causal models.\n\n\n\nDawid’s Response\nDawid admits that to address the responses, “I have to accept that a vital aspect of causal modeling and inference is the identification of modular subprocesses.”\nHe seems to stick to his guns, saying\n\nHowever, I do not feel that the counterfactual approach to causal inference has, as yet, provided any of these advantages.\n\nHe argues the premise of Pearl’s argument about the richness of languages, suggesting that he believes that the Bayesian decision-analytic framework may be an even richer language than that of counterfactual causal inference.\nFinally, he proposes the following framework for evaluating the theory of counterfactual causal inference, asking where on each scale counterfactual causal inference falls:\n\nFact \\(\\leftrightarrow\\) Fiction. Are counterfactuals to be regarded as genuine features of the external world, or are they purely theoretical terms?\nReal \\(\\leftrightarrow\\) Instrumental. Can any inferences based on counterfactuals be allowed, or should they be restricted to those that could in principle be formulated without mention of counterfactuals?\nClear \\(\\leftrightarrow\\) Vague. Do counterfactual terms in a model have a clear relationship with meaningful aspects of the problem addressed? Can counterfactual constructions and arguments help to clarify understanding?\nHelpful \\(\\leftrightarrow\\) Dangerous. Can use of counterfactuals streamline thinking and assist analyses, or do they promote misleading lines of arguments and false conclusions?"
  },
  {
    "objectID": "2000_Dawid/2000_Dawid.html#my-responses",
    "href": "2000_Dawid/2000_Dawid.html#my-responses",
    "title": "4  Causal Inference Without Counterfactuals",
    "section": "My Responses",
    "text": "My Responses\nI’ll bite: let’s think through the dimensions that Dawid proposes, taking into account the ways of thinking that the responses offered as well.\n\nFact or Fiction? If we think about the extent to which the treated vs. untreated arms physically, undeniably demonstrate the possible outcomes with no recourse to metaphysical hypotheses, we can certainly say that when adjustment for confounders doesn’t take the potential outcomes for either group far outside the region of observed data in (covariate, treatment assignment) space, we can certainly refer to the fact-of-the-matter that the counterfactuals posited have support in real-life observed data. Things get trickier in general when we are using models to generate predictions far outside the region of support for which we have observations in (covariate, treatment assignment) space, but in general extrapolation is a hard problem and not a feature of counterfactual causal inference specifically — just something to be careful of.\nReal or Instrumental? Not sure how Dawid intends for anyone to answer this? “Should” counterfactual analyses be allowed? Sure.\nClear vs. Vague? I’d say one of the best things the field of counterfactual causal inference has brought to bear is its emphasis on causal diagram presentation which have rendered clarity to a number of complex social questions, and especially with the rules for d-separation, determining backdoor paths, etc., the lucid clarity that counterfactual causal inference has brought to a wide number of problems is nothing to scoff at.\nHelpful or Dangerous? I suspect whether or not an analysis is helpful or dangerous is independent of whether or not an analyst uses counterfactual causal inference, but rather a feature of the degree to which the analyst harbors dangerous world-views such as those of eugenics, white supremacy, biological essentialism, a disregard for human rights, and comfort with twisting the facts.\n\nFinally, I’d just like to turn Dawid’s argument around on him for just a moment: is it not the case that the predictions necessary for his suggested Bayesian decision-analytic necessitate so-called metaphysical assumptions? For example, one must obtain a prediction, which, if we admit that statistical predictions are uncertain even in best-case scenarios, then we are forced to acknowledge that either:\n\nWe can never validate a single prediction (since we can always resort to saying it was just an exceptionally low-probability outlier in the probability distribution model of the prediction), or\nWe would have to validate it through a sample of repeated observations on the same unit, but of course this suggests that the treatment effect needs to be invariant over time on the same unit.\n\nI think the second statement here is similar to the unit treatment value assumption, where I don’t fully buy that this is an untestable, metaphysical assumption as Dawid claims it is. We can use tools like stratification to test for effect heterogeneity — and broadly speaking, as Pearl points out, either these assumptions are insignificant and hence have no meaningful effect, or they are significant and hence can be observed in their ramifications.\nFinally, I’d like to point out that the truly untestable, metaphysical assumption that Dawid’s decision-analytic framework suffers from in equal measure as the counterfactual causal inference paradigm does is the need to assume that no unmeasured confounders exist. Importantly, it could be a very bad thing if one uses a decision-analytic model that did not capture an important confounder to decide on a treatment policy for an individual and got it wrong."
  },
  {
    "objectID": "2000_Dawid/2000_Dawid.html#conclusions-1",
    "href": "2000_Dawid/2000_Dawid.html#conclusions-1",
    "title": "4  Causal Inference Without Counterfactuals",
    "section": "Conclusions",
    "text": "Conclusions\nWhat have we learned (that felt nontrivial)?\nI particularly like Pearl’s analogizing causal inference to that of learning functions. The framing that, broadly speaking, our posited laws of physics are hypothesized relationships between variables such that if some variables hold particular values, then we would expect to see other variables become determined feels like its taking “counterfactual” causal inference from the domain of speculative language and transforming it into codified statements about what we know about how the universe works.\nI need to think more deeply about the appropriateness of pragmatism in science. On the one hand, it feels reasonable that we should not hold ourselves so rigidly to a philosophical rigor that would cast most science as ineffective. On the other hand, pragmatism is present in ideas I am deeply skeptical of (such as that predictive models need not effectively capture causal relations). Cox’s example where we can imagine observing through perfect randomized controlled trials that a treatment has a statistically significant effect compared to the control, but with no explanatory mechanism detailed, should we act on this knowledge? It might be pragmatic to do so, but as Cox suggests, there is a part of me that feels squeamish about using knowledge founded atop at-best shaky grounds to make important decisions.\nIt’s worth noting that no one involved seemed to change their minds. A few points here and there were conciliatorily granted, but it is not as if either the counterfactualists responding or Dawid admitted defeat. For me, I can only take this as indication that 1) when someone has deeply held beliefs (e.g., science shouldn’t be based on unverifiable, metaphysical quantities; counterfactual causal inference has been working well, etc.) they will rarely let them go, and 2) the significant impact of this paper is in the ways in which it may have helped to shape young readers’ perspectives.\nThere is something valuable here, and Pearl is probably the writer who came closest to explicitly acknowledging it, but it appears that Dawid’s criticisms of counterfactual causal inference has the effect of sharpening the ways the counterfactualists think about their work and how they talk about it. It seems most plausible that when sharply targeted jabs in theory are traded back-and-forth in journal articles between careful, thoughtful colleagues, the typical pupil set to receive the battle-tested treatment of the theory will have a particularly challenging time taking issue with their instructors’ presentation."
  },
  {
    "objectID": "2019_Aronow_Miller/2019_Aronow_Miller.html#introduction",
    "href": "2019_Aronow_Miller/2019_Aronow_Miller.html#introduction",
    "title": "5  Foundations of Agnostic Statistics",
    "section": "Introduction",
    "text": "Introduction\nFor this week’s reading, I jumped straight into the deep-end and read chapters 6 and 7 of Aronow and Miller’s (relatively) new book. I found their text extremely enjoyable and enlightening, and was especially excited to see their easy presentation of doubly-robust estimators, something I’ve been looking around for for a while.\nThe other aspect that I particularly enjoyed was that they make the similarities between handling missing data and questions of causal inference as explicit as possible, going so far as to structure chapters 6 and 7 in parallel, 6 being about missing data and 7 being about causal inference.\nDefinition. The stable outcomes model is for missing data is given \\[Y_i^* = \\left\\{ \\begin{array}{ll} -99 & R_i = 0 \\\\ Y_i & R_i = 1 \\end{array} \\right.\\] where \\(R_i\\) is the missingness indicator variable. This can also be written as \\(Y_i^* = Y_i R_i + (-99)(1-R_i)\\).\nThis is called the stable outcomes model because it presumes that \\(Y_i\\) is itself a stable quantity.\nAs far as I can tell, they’re using \\(-99\\) in the equations not because it makes the math go through, but because \\(-99\\) is number commonly used to indicate missingness."
  },
  {
    "objectID": "2019_Aronow_Miller/2019_Aronow_Miller.html#sharp-bounds-for-estimates",
    "href": "2019_Aronow_Miller/2019_Aronow_Miller.html#sharp-bounds-for-estimates",
    "title": "5  Foundations of Agnostic Statistics",
    "section": "Sharp Bounds for Estimates",
    "text": "Sharp Bounds for Estimates\n\nMissing Data\nTheorem 6.1.3. Sharp bounds for the expected value. Let \\(Y_i\\) and \\(R_i\\) be random variables with support for \\(Y_i\\) be restricted to \\([a,b]\\), \\(\\text{support}(R_i) \\in \\{0,1\\}\\), and \\(Y_i*\\) is as in the stable outcomes model. Then\n\\[\\begin{align}\n\\mathbb E[Y_i] \\in \\big[\n\\mathbb E[Y_i^* | & R_i = 1]Pr(R_i=1) + aPr(R_i = 0), \\\\\n& \\mathbb E[Y_i^* | R_i = 1]Pr(R_i=1) + bPr(R_i = 0)\\big]. \\\\\n\\end{align}\\]\nI found this books’ proofs quite clear and easy to follow, so I am not going to re-write them here and instead refer readers to the book.\nInstead, I think some simulation exercise to build our intuition on this would be nice.\n\nN <- 1000\nA <- rnorm(n = N)\n\n# introduce the logistic function\nlogistic <- function(x) { exp(x) / (1+exp(x)) }\ncurve(logistic(x), from = -10, to = 10)\n\n\n\n# simulate our true outcome\nY <- rnorm(n = N, mean = 2*A + A^2)\n\n# simulate our missingness mechanism\nR <- rbinom(n = N, size = 1, prob = logistic(A+Y))\n\n# subset observed data\nA_observed <- A[as.logical(1-R)]\nY_observed <- Y[as.logical(1-R)]\n\nplot(A, Y)\n\n\n\nplot(A_observed, Y_observed)\n\n# estimate true regression equation\njtools::summ(lm(Y ~ A))\n\n\n\n\n\n\n\n  \n    Observations \n    1000 \n  \n  \n    Dependent variable \n    Y \n  \n  \n    Type \n    OLS linear regression \n  \n\n \n\n  \n    F(1,998) \n    1304.73 \n  \n  \n    R² \n    0.57 \n  \n  \n    Adj. R² \n    0.57 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    t val. \n    p \n  \n \n\n  \n    (Intercept) \n    1.06 \n    0.06 \n    18.55 \n    0.00 \n  \n  \n    A \n    2.02 \n    0.06 \n    36.12 \n    0.00 \n  \n\n\n Standard errors: OLS\n\n\n# missingness biased regression equation\njtools::summ(lm(Y_observed ~ A_observed))\n\n\n\n\n  \n    Observations \n    431 \n  \n  \n    Dependent variable \n    Y_observed \n  \n  \n    Type \n    OLS linear regression \n  \n\n \n\n  \n    F(1,429) \n    8.43 \n  \n  \n    R² \n    0.02 \n  \n  \n    Adj. R² \n    0.02 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    t val. \n    p \n  \n \n\n  \n    (Intercept) \n    -0.46 \n    0.07 \n    -6.36 \n    0.00 \n  \n  \n    A_observed \n    0.22 \n    0.07 \n    2.90 \n    0.00 \n  \n\n\n Standard errors: OLS\n\n\n# create stable_outcomes_model\nY_stable <- Y*R + (-99)*(1-R)\n\n# lower bound on the A-Y effect\nmean(Y_stable*R + min(Y_observed)*(1-R))\n\n[1] -0.4937127\n\n# upper bound on the A-Y effect\nmean(Y_stable*R + max(Y_observed)*(1-R))\n\n[1] 2.803694\n\n\n\n\nCausal Estimation\nIn chapter 7, they provide a similar theorem that provides the means to bound a causal effect. There is one crucial difference, though, which is that because the average treatment effect is the difference between two potential outcomes (under treatment and under non-treatment), whereas the effect estimate in the missing data scenario is purely associational, the causal theorem has an additional subtracted term.\nTheorem 7.1.17. Sharp bounds for the average treatment effect.\nLet \\(Y_i(0)\\), \\(Y_i(1)\\), and \\(D_i\\) be random variables such that \\(\\forall d \\in \\{0, 1\\},\\) \\(\\text{support}(Y_i(d)) \\subset [a,b],\\) and \\(\\text{support}(D_i) = \\{0,1\\}\\). Let \\(Y_i = Y_i(1) \\cdot D_i + Y_i(0) \\cdot (1-D_i)\\) and \\(\\tau_i = Y_i(1) - Y_i(0)\\). Then\n\\[\\begin{aligned}\\mathbb E[\\tau_i] \\in \\big[ \\mathbb E( & Y_i | D_i = 1)Pr(D_i=1) + aPr(D_i = 0) \\\\\n&  -(\\mathbb E(Y_i|D_i=0)Pr(D_i=0) + bPr(D_i=1)), \\\\\n& \\mathbb E( Y_i | D_i = 1)Pr(D_i=1) + bPr(D_i = 0) \\\\\n& -(\\mathbb E(Y_i|D_i=0)Pr(D_i=0) + aPr(D_i=1)) \\big]. \\end{aligned} \\]\nLet’s simulate some data and run an analysis to build some intuition around how this sharp bounding works. For example, we might imagine a scenario where the treatment assigment and the outcome are confounded, and hence the naive (unadjusted) estimate of the average treatment effect would be biased.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nN <- 1000\n\nset.seed(1234)\nL <- rnorm(n = N)\nD <- rbinom(n = N, size = 1, prob = logistic(L))\nY1 <- L + rnorm(n = N, mean = 2) # outcome under treatment\nY0 <- L + rnorm(n = N, mean = 0) # outcome under no treatment\n\nY <- Y1*D + Y0*(1-D)\n\nunadjusted_model <- lm(Y ~ D)\n\nggplot(data.frame(D = as.factor(D), Y = Y),\n       aes(x = D, y = Y, color = D)) + \n  ggforce::geom_sina(alpha = 0.35, scale = 'width', maxwidth=.5) + \n  geom_boxplot(alpha = 0.5, width = .3, outlier.color = NA, color = 'black') + \n  stat_summary(fun = mean, geom = 'point', size = 3, shape = 1, color = 'black') + \n  theme_bw() + \n  theme(legend.position = 'bottom') + \n  ggtitle(\"D (treatment) and Y (outcome) are confounded by (unshown) L\",\n          \"Averages are indicated by the open black circles\") +\n  labs(caption = paste0(\"The biased estimate of average treatment effect is roughly \", round(coef(unadjusted_model)[2], 1)))\n\n\n\n\n\n\n\n\n\nggplot(data.frame(L = L, D = as.factor(D), Y = Y),\n       aes(x = L, y = Y, color = D)) + \n  geom_point(alpha = 0.25) + \n  geom_smooth(method = 'lm', se=F) + \n  theme_bw() + \n  theme(legend.position = 'bottom') + \n  labs(color = 'D (treatment):') + \n  scale_y_continuous(breaks = c(-4,-2,0,2,4,6)) + \n  ggtitle(\"Conditioning on L allows us to see that the average treatment effect is actually 2\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nSo now can we check if the sharp bounds contain 2, which they should as long as the theorem holds? (check the book for the proof!)\n\nlower_bound <- mean(Y*D + min(Y)*(1-D) - Y*(1-D) - max(Y)*D)\nupper_bound <- mean(Y*D + max(Y)*(1-D) - Y*(1-D) - min(Y)*D)\n\nprint(\"the true average treatment effect should be guaranteed to lie within:\")\n\n[1] \"the true average treatment effect should be guaranteed to lie within:\"\n\nprint(paste0(\"[\", round(lower_bound, 1), ', ', round(upper_bound, 1), \"]\"))\n\n[1] \"[-4.1, 6.9]\"\n\nprint(\"is the actual average treatment effect within these bounds?\")\n\n[1] \"is the actual average treatment effect within these bounds?\"\n\nif (2 >= lower_bound & 2 <= upper_bound) print(\"yup! 🙂\") else print(\"nope 😕\")\n\n[1] \"yup! 🙂\""
  },
  {
    "objectID": "2019_Aronow_Miller/2019_Aronow_Miller.html#inverse-probability-weighting",
    "href": "2019_Aronow_Miller/2019_Aronow_Miller.html#inverse-probability-weighting",
    "title": "5  Foundations of Agnostic Statistics",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\n\nMissing Data\nThe inverse probability weighting for missing data described in theorem 6.2.6 for missing at random data is exactly the same as the propensity score weighting described in Rosenbaum and Rubin’s paper.\nWhat’s more interesting to me is to see that they provide a stabilized inverse probability weighting approach for missing data.\nDefinition 6.2.8 Stabilized IPW Estimator for Missing Data\nLet \\(Y_i\\) and \\(R_i\\) be random variables with \\(\\text{support}(R_i) = \\{ 0, 1 \\}\\). Let \\(Y_i^*\\) be as in the stable outcomes model and let \\(X_i\\) be a random vector. Then given \\(n\\) independently and identically distributed observations of \\((Y_i^*, R_i, X_i)\\), the stabilized IPW estimator for \\(\\mathbb E[Y_i]\\) is\n\\[ \\hat{\\mathbb E}_{SIPW} (Y_i) = \\frac{\\frac{1}{n} \\sum_{i=1}^n \\frac{Y_i R^i}{\\hat p_R(X_i)}}{\\frac{1}{n} \\sum_{i=1}^n \\frac{R^i}{\\hat p_R(X_i)}}. \\]\n\n\nCausal Estimation\nA similar stabilization approach is given for causal average treatment effects:\nDefinition 7.2.7. Stabilized IPW Estimator for Causal Inference. Let \\(Y_i(0)\\), \\(Y_i(1)\\), and \\(D_i\\) be random variables with \\(\\text{support}(D_i) = \\{0,1\\}\\). Let \\(Y_i = Y_i(1) \\cdot D_i + Y_i(0) \\cdot (1-D_i)\\) and \\(\\tau_i = Y_i(1) - Y_i(0)\\), and let \\(X_i\\) be a random vector. Then given \\(n\\) i.i.d. observations of \\((Y_i, D_i, X_i)\\), the stabilized IPW estimator for \\(\\mathbb E[\\tau_i]\\) is\n\\[\\hat{\\mathbb E}_{SIPW}[\\tau_i] = \\frac{\n\\frac{1}{n} \\sum_{i=1}^n \\frac{Y_iD_i}{\\hat p_{D}(X_i)\n}}{\n\\frac{1}{n} \\sum_{i=1}^n \\frac{D_i}{\\hat p_{D}(X_i)\n}} -\n\\frac{\n\\frac{1}{n} \\sum_{i=1}^n \\frac{Y_i(1-D_i)}{1 -\\hat p_{D}(X_i)\n}}{\n\\frac{1}{n} \\sum_{i=1}^n \\frac{1-D_i}{1-\\hat p_{D}(X_i)\n}}.\n\\]"
  },
  {
    "objectID": "2019_Aronow_Miller/2019_Aronow_Miller.html#doubly-robust-estimation",
    "href": "2019_Aronow_Miller/2019_Aronow_Miller.html#doubly-robust-estimation",
    "title": "5  Foundations of Agnostic Statistics",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\n\nMissing Data\nTheorem 6.2.9 Double Robustness Theorem for Missing Data. Let \\(Y_i\\), \\(Y_i^*\\), \\(R_i\\), \\(X_i\\) be as in definition 6.2.8 above. If missing at randomness holds and if either\n\n\\(\\hat{\\mathbb E}[Y_i^*|R_i=1,X_i=x] = \\mathbb E[Y_i^*|R_i=1,X_i=x]\\) for all \\(x \\in \\text{support}(X_i)\\) and \\(\\exists \\epsilon > 0\\) such that \\(\\epsilon < \\hat p_R(x) < 1-\\epsilon\\), or\n\\(\\hat p_R(x) = p_R(x)\\) for all \\(x \\in \\text{support}(X_i)\\),\n\nthen:\n\\[\\mathbb E[Y_i] = \\mathbb E\\left[ \\hat{\\mathbb E}[Y_i^*|R_i=1,X_i] + \\frac{R_i\\left(Y_i^* - \\hat{\\mathbb E}[Y_i^* | R_i=1,X_i] \\right)}{\\hat p_R(x)} \\right].\\]\nWhat’s interesting about the proof is that it essentially follows in a straightforward fashion by assuming (case-by-case) that either 1 or 2 hold and seeing that the other terms cancel away leaving us with the term assumed to equal \\(\\mathbb E[Y_i]\\) in both cases.\nDefinition 6.2.10 Doubly Robust Estimator for Missing Data Let \\(Y_i\\), \\(Y_i^*\\), \\(R_i\\) and \\(X_i\\) be as in definition 6.2.8. Then the doubly robust estimator is given as\n\\[ \\hat{\\mathbb E}_{DR}[Y_i] = \\frac{1}{n} \\sum_{i=1}^n \\hat{\\mathbb E}[Y_i^* | R_i = 1, X_i] +\n\\frac{1}{n} \\sum_{i=1}^n \\frac{R_i \\left(Y_i^* - \\hat{\\mathbb E}[Y_i^* | R_i = 1, X_i]\\right)}{\\hat p_R(X_i) }.\\]\n\n\nCausal Estimation\nTurning our attention to causal inference, we have a similar double robustness theorem.\nTheorem 7.2.8. Double Robustness Theorem for Causal Inference. Let \\(Y_i\\), \\(D_i\\) \\(\\tau_i\\) and \\(X_i\\) be as in definition 7.2.7. If strong ignorability holds and if either\n\n\\(\\hat{\\mathbb E}[Y_i|D_i=d,X_i=x] = \\mathbb E[Y_i|D_i=d,X_i=x]\\) for all \\(d \\in \\{0,1\\}\\), \\(x \\in \\text{support}(X_i)\\) and \\(\\exists \\epsilon > 0\\) such that \\(\\epsilon < \\hat{p}_D(x) < 1-\\epsilon\\), or\n\\(\\hat{p}_D(x) = p_D(x)\\), for all \\(x \\in \\text{support}(X_i)\\),\n\nthen:\n\\[ \\begin{aligned}\\mathbb E[\\tau_i] = \\mathbb E\\Big[\n\\hat{\\mathbb E}[Y_i|& D_i=1,X_i] + \\frac{D_i(Y_i - \\hat{\\mathbb E}[D_i=1,X_i])}{\n\\hat p_D(X_i)} \\\\\n& -\\hat{\\mathbb E}[Y_i|D_i=0,X_i] - \\frac{(1-D_i)(Y_i - \\hat{\\mathbb E}[Y_i|D_i=0,X_i])}{1-\\hat{p}_D(X_i)}\\Big]. \\end{aligned} \\]\nDefinition 7.2.9 Doubly Robust Estimator for Causal Inference. Again, let \\(Y_i\\), \\(D_i\\) \\(\\tau_i\\) and \\(X_i\\) be as in definition 7.2.7. The doubly robust estimator is\n\\[\\begin{aligned}\\hat{\\mathbb E}_{DR}[\\tau_i] = & \\frac{1}{n} \\sum_{i=1}^n \\hat{\\mathbb E}[Y_i|D_i=1,X_i] +\n\\frac{1}{n}\\sum_{i=1}^n \\frac{D_i(Y_i - \\hat{\\mathbb E}[Y_i |D_i=1,X_i])}{\\hat p_D(X_i)} \\\\\n& - \\frac{1}{n}\\sum_{i=1}^n [Y_i|D_i=0,X_i] -\n\\frac{1}{n}\\sum_{i=1}^n \\frac{(1-D_i)(Y_i - \\hat{\\mathbb E}[Y_i |D_i=0,X_i])}{1-\\hat p_D(X_i)}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html",
    "href": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html",
    "title": "3  Marginal Structural Models",
    "section": "",
    "text": "4 Simulations to Build Intuition\nI’ve been motivated by the question “how can we tell that we’ve implemented our marginal structural model (MSM) correctly if we allow ourselves to check our work in a simulation setting?”\nMy idea for “checking” centers around the idea that we can simulate data with varying strength on the pathways from the \\(L\\) (the time-varying confounders affected by the exposures) variables to the \\(A\\) (exposure) variables and compare what happens in MSMs comparing to the setting where there was 0 confounding.\nTo come up with a more straightforward setting to test our ideas in, the following DAG is sufficient as an example of a scenario where time-varying-confounding affected by exposures exist:\nWe’ll introduce a few functions to simulate some data.\nJust to check that everything is working right, we might do one simulation alone:\nNow we can finally investigate whether or not my intuition is right:\nWhat’s quite interesting to me is that when we start to ramp up the confounding path strength to be higher than 1, we start to run into scenarios where the variability in the \\(A_0\\) estimate after IPTW is quite high and we fail to be able to continue to recover unbiased estimates of the \\(A_0\\) and \\(A_1\\) coefficients."
  },
  {
    "objectID": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#simulations-to-build-intuition",
    "href": "2000_Robins_Hernan_and_Brumback/2000_Robins_Hernan_and_Brumback.html#simulations-to-build-intuition",
    "title": "3  Marginal Structural Models",
    "section": "Simulations to Build Intuition",
    "text": "Simulations to Build Intuition\nI’ve been motivated by the question “how can we tell that we’ve implemented our marginal structural model (MSM) correctly if we allow ourselves to check our work in a simulation setting?”\nMy idea for “checking” centers around the idea that we can simulate data with varying strength on the pathways from the \\(L\\) (the time-varying confounders affected by the exposures) variables to the \\(A\\) (exposure) variables and compare what happens in MSMs comparing to the setting where there was 0 confounding.\nTo come up with a more straightforward setting to test our ideas in, the following DAG is sufficient as an example of a scenario where time-varying-confounding affected by exposures exist:\n\n\n\n\n\nA simpler situation with time-varying confounders affected by exposures.\n\n\n\n\nWe’ll introduce a few functions to simulate some data.\n\n# Function to generate simulated data\ngenerate_sim_data <- function(N, confounding_path_strength) {\n  L0 <- rnorm(N)\n  A0 <- rnorm(N, mean = L0 * confounding_path_strength)\n  L1 <- rnorm(N, mean = L0 + A0)\n  A1 <- rnorm(N, mean = A0 + L0 * confounding_path_strength + L1 * confounding_path_strength)\n  Y <- rnorm(N, mean = A0 + A1 + L1)\n  \n  data.frame(L0 = L0, A0 = A0, L1 = L1, A1 = A1, Y = Y)\n}\n\n# Function to calculate stabilized IPTW\n# Note that since the above data generating function uses continuous variables, we \n# estimate the densities of observations\ncalculate_stable_weights <- function(data) {\n  model_A0 <- lm(A0 ~ L0, data = data)\n  model_A1 <- lm(A1 ~ A0 + L0 + L1, data = data)\n  \n  weights_A0 <-\n    1 / dnorm(\n      x = data$A0,\n      mean = coef(model_A0)[1],\n      sd = summary(model_A0)$sigma\n    )\n  weights_A1 <- 1 / dnorm(\n    x = data$A1,\n    mean = coef(model_A1)[1] + coef(model_A1)[2] * data$A0 +\n      coef(model_A1)[3] * data$L0 + coef(model_A1)[4] * data$L1,\n    sd = summary(model_A1)$sigma\n  )\n  \n  model_A0_alone <- lm(A0 ~ 1, data = data)\n  model_A1_stabilizer <- lm(A1 ~ A0, data = data)\n  A0_stabilizer <-\n    dnorm(\n      x = data$A0,\n      mean = coef(model_A0_alone)[1],\n      sd = summary(model_A0_alone)$sigma\n    )\n  A1_stabilizer <-\n    dnorm(\n      x = data$A1,\n      mean = coef(model_A0_alone)[1] + data$A0 * coef(model_A1_stabilizer)[2],\n      sd = summary(model_A1_stabilizer)$sigma\n    )\n  \n  A0_stabilizer * A1_stabilizer * weights_A0 * weights_A1\n}\n\nJust to check that everything is working right, we might do a pair of simulations to compare:\n\n# Generate simulated data\nN <- 1000\nconfounding_path_strength <- 1\nsim_data <- generate_sim_data(N, confounding_path_strength)\n\n# Calculate weights\nsim_data$stable_weights <- calculate_stable_weights(sim_data)\n\n# Unadjusted model\nmodel_unadjusted <- lm(Y ~ A0 + A1, data = sim_data)\njtools::summ(model_unadjusted, model.info=FALSE, model.fit = FALSE)\n\n\n\n\n\n\n\n\nEst.\n\n\nS.E.\n\n\nt val.\n\n\np\n\n\n\n\n\n\n(Intercept)\n\n\n0.00\n\n\n0.04\n\n\n0.05\n\n\n0.96\n\n\n\n\nA0\n\n\n1.08\n\n\n0.07\n\n\n16.21\n\n\n0.00\n\n\n\n\nA1\n\n\n1.47\n\n\n0.02\n\n\n72.42\n\n\n0.00\n\n\n\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n# Adjust for confounders using MSM\nmodel_msm <- lm(Y ~ A0 + A1, data = sim_data, weights = sim_data$stable_weights)\njtools::summ(model_msm, model.info=FALSE, model.fit = FALSE)\n\n\n\n\n\n\n\n\nEst.\n\n\nS.E.\n\n\nt val.\n\n\np\n\n\n\n\n\n\n(Intercept)\n\n\n-0.29\n\n\n0.05\n\n\n-6.00\n\n\n0.00\n\n\n\n\nA0\n\n\n1.38\n\n\n0.11\n\n\n12.58\n\n\n0.00\n\n\n\n\nA1\n\n\n1.30\n\n\n0.03\n\n\n37.27\n\n\n0.00\n\n\n\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\nIf the above estimates are showing us that in the marginal structural model with IPTW we’re getting coefficients on \\(A_0\\) of roughly 2 and coefficients on \\(A_1\\) of roughly 1, that would make a lot of sense. If we consider the DAG where we’re controlling for \\(A_1\\) (as in the regression performed for model_msm), we can see that there’s two pathways (shown in blue below) through which \\(A_0\\) contributes to the outcome \\(Y\\), and only the one direct pathway from \\(A_1\\) to \\(Y\\) (shown in red).\n\n\n\n\n\nPathways from the exposure to the outcome once we remove the arrows from \\(L_{0,1} \\to A_{0,1}\\).\n\n\n\n\nNow we can finally investigate whether or not my intuition is right:\n\n# Look at estimates when confounding_path_strength is ramped up or down \nresults <- list()\nfor (confounding_path_strength in seq(-1,1,length.out = 7)) {\n  for (i in 1:100) {\n  # Generate simulated data\n  N <- 1000\n  sim_data <- generate_sim_data(N, confounding_path_strength)\n  \n  # Calculate weights\n  sim_data$stable_weights <- calculate_stable_weights(sim_data)\n  \n  # Unadjusted model\n  model_unadjusted <- lm(Y ~ A0 + A1, data = sim_data)\n\n  # Adjust for confounders using MSM\n  model_msm <- lm(Y ~ A0 + A1, data = sim_data, weights = sim_data$stable_weights)\n\n  # store results from MSM model\n  results[[length(results)+1]] <- \n    list(confounding_path_strength = confounding_path_strength,\n      i = i,\n      model = 'marginal structural model (with IPTW)',\n      intercept = unname(coef(model_msm)[1]),\n      A0_effect = unname(coef(model_msm)[2]),\n      A1_effect = unname(coef(model_msm)[3]))\n  \n  # store results from unadjusted model\n  results[[length(results)+1]] <- \n    list(confounding_path_strength = confounding_path_strength,\n      i = i,\n      model = 'unadjusted',\n      intercept = unname(coef(model_unadjusted)[1]),\n      A0_effect = unname(coef(model_unadjusted)[2]),\n      A1_effect = unname(coef(model_unadjusted)[3]))\n  }\n}\n\n# turn into a dataframe\nresults <- bind_rows(results)\n\nlabelling_helper <- function(x) { \n  paste0(\"confounding path coefficient: \", round(as.numeric(x), 1))\n}\n\nresults |> \n  tidyr::pivot_longer(cols = c(intercept, A0_effect, A1_effect), names_to = 'coefficient', values_to = 'estimate') |> \n  ggplot(aes(\n    x = coefficient,\n    y = estimate,\n    color = model,\n    group = model\n  )) +\n  geom_jitter(height = 0, width=.15, alpha = .15) + \n  stat_summary(\n    fun = mean,\n    geom = 'point',\n    aes(group = model, shape = model),\n    color = 'grey30'\n  ) +\n  stat_summary(fun = mean, geom = 'line', aes(group = model)) +\n  facet_wrap(\n    ~ confounding_path_strength,\n    nrow = 1,\n    labeller = as_labeller(labelling_helper)\n  ) +\n  theme_bw() +\n  theme(\n    legend.position = 'bottom',\n    axis.text.x = element_text(angle = 15, hjust = 1),\n    plot.caption = element_text(hjust = .5)\n  ) +\n  ggtitle(\"Variation in Coefficient Estimates with/without IPTW\",\n          subtitle = \"Datasets (N obs=1000) were simulated for varying coefficients for the confounding paths (L0→A0, L0→A1, L1→A1) 100 times\") +\n  labs(caption = \"Mean coefficient estimates from the 100 simulations are indicated by the black marks that are connected by colored lines for each model type.\")\n\n\n\n\n\n\nWhat’s quite interesting to me is that when we start to ramp up the confounding path strength to be higher than 1, we start to run into scenarios where the variability in the \\(A_0\\) estimate after IPTW is quite high and we fail to be able to continue to recover unbiased estimates of the \\(A_0\\) and \\(A_1\\) coefficients."
  }
]